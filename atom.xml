<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>KxZhang&#39;s Blog</title>
  
  <subtitle>Like a giddy planet beside a burning star.</subtitle>
  <link href="https://erenjaeger-01.github.io/atom.xml" rel="self"/>
  
  <link href="https://erenjaeger-01.github.io/"/>
  <updated>2022-10-28T03:00:52.000Z</updated>
  <id>https://erenjaeger-01.github.io/</id>
  
  <author>
    <name>张凯翔</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【毕设系列】语义分割（一）</title>
    <link href="https://erenjaeger-01.github.io/2022/10/28/%E3%80%90%E6%AF%95%E8%AE%BE%E7%B3%BB%E5%88%97%E3%80%91%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://erenjaeger-01.github.io/2022/10/28/%E3%80%90%E6%AF%95%E8%AE%BE%E7%B3%BB%E5%88%97%E3%80%91%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%EF%BC%88%E4%B8%80%EF%BC%89/</id>
    <published>2022-10-28T01:45:46.000Z</published>
    <updated>2022-10-28T03:00:52.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h3><h4 id="语义分割、实例分割、全景分割"><a href="#语义分割、实例分割、全景分割" class="headerlink" title="语义分割、实例分割、全景分割"></a>语义分割、实例分割、全景分割</h4><ul><li>语义分割：只区分类别，不区分个体。简单来说语义分割可以把一幅图像中的人全抠出来，但是对于这些人中的每一个单独的个体并不能够区分出来。</li><li>实例分割：只关注前景（可以计数的物体）的分割，并且对物体加以区分。</li><li>全景分割：区分前景和背景，前景进行示例分割，背景（比如道路、天空）进行语义分割。</li></ul><p><img src="/./%E3%80%90%E6%AF%95%E8%AE%BE%E7%B3%BB%E5%88%97%E3%80%91%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%EF%BC%88%E4%B8%80%EF%BC%89/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2%E5%85%A8%E6%99%AF%E5%88%86%E5%89%B2%E5%8C%BA%E5%88%AB.png" alt="三者之间的区别"></p><h4 id="主要性能指标"><a href="#主要性能指标" class="headerlink" title="主要性能指标"></a>主要性能指标</h4><ul><li>Dice coefficient：集合相似度度量系数</li><li>mIou：平均交并比</li></ul><h4 id="上采样和下采样"><a href="#上采样和下采样" class="headerlink" title="上采样和下采样"></a>上采样和下采样</h4><ul><li>上采样：又名放大图像、图像插值。主要目的是放大原图像，从而可以显示在更高分辨率的设备上。<br>常用方法有双线性插值，反卷积，反池化等。</li><li>下采样：又名降采样，缩小图像。主要目的一是使图像符合显示区域的大小，二是生成对应图像的缩略图（其实就是池化）。</li></ul><h3 id="论文浏览"><a href="#论文浏览" class="headerlink" title="论文浏览"></a>论文浏览</h3><p>主要看的实时语义分割，快就是好</p><h4 id="ENet"><a href="#ENet" class="headerlink" title="ENet"></a>ENet</h4><p>16年的文献，在当时做的还是很好，论文上说速度比之前的算法快了18倍，算力和参数量需求都减少了70多倍，效果和之前的算法差不多。</p><p>总结：在普通语义分割模型的的基础上使用了分解卷积、空洞卷积、轻量化decoder、减小下采样次数等操作提升速度，但是减少下采样对大物体识别分割造成影响，降低精度。</p><p>一些Trick：</p><ul><li><p>减少下采样和上采样过程的分割精度丢失：<br>解决下采样中边缘信息丢失问题的主流方法有2种，一是FCN中的添加编码层的feature map，二是Segnet中通过保留编码网络中最大池化过程中最大值的索引，并借此在解码网络中生成稀疏的上采样feature map。ENet采用了SegNet中的方式来降低下采样和上采样过程的分割精度丢失。</p></li><li><p>非对称的Encoder-Decoder结构，降低参数量：<br>ENet网络结构是非对称的，包含一个较大的编码层和一个小的解码网络。原因是编码层应该像原始分类网络的结构相似。用于处理较小的数据，同时进行信息的处理与滤波，解码网络对编码网络的输出进行上采样用于对细节的微调。</p></li><li><p>激活函数使用PReLU，而非ReLU：<br>作者实验发现去掉网络初始层中的大部分ReLU层会提升分割的效果，原因是此网络的深度不够深。然后，该文将所有ReLU替换为PReLUs，针对每张feature map增加了一个额外的参数。得到了精度的提升。</p></li><li><p>分解卷积核：<br>Inception Net 的套路，一个二维的卷积可以被分解为两个维度为1的卷积核。论文使用5x1,1x5的非对称卷积，减弱了函数学习的过拟合同时增加了感受野。卷积核的分解带来的另一个好处是可以减少大量的参数。同时加上更多的非线性处理，使计算功能更加丰富。</p></li><li><p>使用空洞卷积：<br>论文将bottleneck中的卷积层替换为空洞卷积并进行串联，增大了感受野，提高了分割的IOU。</p></li><li><p>正则化：<br>由于分割数据集有限，因此网络的训练很容易达到过拟合。该文使用空间Dropout进行处理。</p></li><li><p>信息保留的维度变化：<br>前半部分进行下采样是有必要的，但剧烈的维度衰减不利于信息的流动。为解决这个问题，该文在池化层后接一个卷积层增加了维度，进而增加了计算资源。为此，该文将池化操作与卷积操作进行并行操作，然后进行拼接。同时，在原始的ResNet的结构中，进行下采样时，第一个1x1的映射，在所有维度上进行的是步长为2的卷积，丢弃了75%左右的输入信息。将卷积核增加至2x2利于信息的保留。</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;center&gt;到目前为止还是想做一个闪亮的毕设的&lt;/center&gt;</summary>
    
    
    
    <category term="Computer Version" scheme="https://erenjaeger-01.github.io/categories/Computer-Version/"/>
    
    
    <category term="Computer Version" scheme="https://erenjaeger-01.github.io/tags/Computer-Version/"/>
    
  </entry>
  
  <entry>
    <title>控制之美</title>
    <link href="https://erenjaeger-01.github.io/2022/08/28/%E6%8E%A7%E5%88%B6%E4%B9%8B%E7%BE%8E/"/>
    <id>https://erenjaeger-01.github.io/2022/08/28/%E6%8E%A7%E5%88%B6%E4%B9%8B%E7%BE%8E/</id>
    <published>2022-08-28T03:18:47.000Z</published>
    <updated>2022-08-28T07:29:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>截取了其中几页</p><p><img src="/./%E6%8E%A7%E5%88%B6%E4%B9%8B%E7%BE%8E/1.png"><br><img src="/./%E6%8E%A7%E5%88%B6%E4%B9%8B%E7%BE%8E/2.png"><br><img src="/./%E6%8E%A7%E5%88%B6%E4%B9%8B%E7%BE%8E/3.png"><br><img src="/./%E6%8E%A7%E5%88%B6%E4%B9%8B%E7%BE%8E/4.png"><br><img src="/./%E6%8E%A7%E5%88%B6%E4%B9%8B%E7%BE%8E/5.png"><br><img src="/./%E6%8E%A7%E5%88%B6%E4%B9%8B%E7%BE%8E/6.png"></p>]]></content>
    
    
    <summary type="html">隔离期间看的一本很有意思的书。</summary>
    
    
    
    <category term="杂记" scheme="https://erenjaeger-01.github.io/categories/%E6%9D%82%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>PX4学习（一）：倾转分离</title>
    <link href="https://erenjaeger-01.github.io/2022/07/15/PX4%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%80%BE%E8%BD%AC%E5%88%86%E7%A6%BB/"/>
    <id>https://erenjaeger-01.github.io/2022/07/15/PX4%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%80%BE%E8%BD%AC%E5%88%86%E7%A6%BB/</id>
    <published>2022-07-15T01:23:09.000Z</published>
    <updated>2022-07-16T07:12:32.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="起源"><a href="#起源" class="headerlink" title="起源"></a>起源</h3><p>多旋翼的姿态控制中，$roll$和$pitch$的改变来自于旋翼提供的升力差实现，调整很快，十几毫秒就能到位，$yaw$的改变是通过改变旋翼产生的反扭距来调成，调整时间比较慢，要快100个毫秒才能就位。</p><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>一般来说，多旋翼80%的能量用于抵抗重力，20%的能量用于控制姿态。当$roll，pitch，yaw$都有较大误差时，20%的能量被三个通道的控制器共同使用，而由于$yaw$响应较慢，会导致$yaw$的误差减小的比较慢，占用了更多用于姿态控制的能量，反而影响到整个姿态控制。</p><h3 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h3><p>因为旋翼飞行器稳定飞行的第一要求是桨平面的精确控制，也就是$roll$和$pitch$，这两个角控不好飞机飞都飞不稳，相对来说，$yaw$是不是足够精确显得没有那么重要（毕竟机头朝向哪里都能飞）。那么在计算误差时，把误差分为两个部分，一部分是倾斜误差$tilt$（$roll$和$pitch$误差），一部分是旋转误差$torsion$，然后对$torsion$做一些处理，比如限幅或者衰减，再重新合成一个总的姿态误差给控制器，这样，大部分姿态控制的能量用于$roll$和$pitch$，有助于实现稳定飞行。</p><h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><ul><li>对齐当前机体坐标系z轴和期望机体坐标系z轴得到一个新的坐标系，获得tilt误差</li><li>把$tilt$误差转换到地理坐标系或者机体坐标系</li><li>总误差-$tilt$误差$&#x3D;torsion$误差</li><li>限制$torsion$误差</li><li>把限制后的$torsion$误差和$tilt$误差合成新的误差作为总误差</li></ul><h3 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h3><h4 id="四元数"><a href="#四元数" class="headerlink" title="四元数"></a>四元数</h4><p>首先定义两个向量</p><ul><li>$v_b$：某一个向量在机体坐标系下的坐标</li><li>$v_e$：同一个向量在地理坐标系下的坐标</li></ul><p>那么旋转过程可以用下面的公式来表示：</p><p>$$<br>   v_e &#x3D; q_b^e \cdot v_b \cdot (q_b^e)^{-1}<br>$$</p><p>这里，$q_b^e$就对应旋转矩阵中的$C_b^e$。</p><p>同时，四元数还遵循链式法则，比如某个向量连续进行了两次旋转，那么有</p><p>$$<br>    q^3_1 &#x3D; q^3_2 \cdot q^2_1<br>$$</p><p><strong>注意</strong> 链式法则只有在同一坐标系下才是可以用的，比如机体坐标系，首先旋转了$q1$，然后又旋转了$q2$，那么总的旋转可以用$q&#x3D;q2 \cdot q1$来表示，再转到地理坐标系下就是</p><p>$$<br>    v_e &#x3D; q2 \cdot (q1 \cdot v_b \cdot (q1)^{-1}) \cdot (q2)^{-1} &#x3D; (q2 \cdot q1) \cdot v_b \cdot (q2 \cdot q1)^{-1}<br>$$</p><h4 id="旋转矩阵"><a href="#旋转矩阵" class="headerlink" title="旋转矩阵"></a>旋转矩阵</h4><p>先定义几个概念：当前坐标系下的一组单位正交基<br>$$<br>    \begin{pmatrix}<br>    e_1 &amp; e_2 &amp; e_3<br>    \end{pmatrix}<br>$$<br>经过旋转后，新的坐标系下的单位正交基为<br>$$<br>    \begin{pmatrix}<br>    e^{‘}_1 &amp; e^{‘}_2 &amp; e^{‘}_3<br>    \end{pmatrix}<br>$$<br>对于同一向量$a$，在两个坐标系下的坐标分别为<br>$$<br>    \begin{pmatrix}<br>    a_1 &amp; a_2 &amp; a_3<br>    \end{pmatrix}^T ,<br>    \begin{pmatrix}<br>    a^{‘}_1 &amp; a^{‘}_2 &amp; a^{‘}_3<br>    \end{pmatrix}^T<br>$$</p><p>那么通过基与坐标的关系可得<br>$$<br>    \begin{pmatrix}<br>    e_1 &amp; e_2 &amp; e_3<br>    \end{pmatrix} \cdot<br>    \begin{pmatrix}<br>    a_1 \ a_2 \ a_3<br>    \end{pmatrix} &#x3D;<br>    \begin{pmatrix}<br>    e^{‘}_1 &amp; e^{‘}_2 &amp; e^{‘}_3<br>    \end{pmatrix} \cdot<br>    \begin{pmatrix}<br>    a^{‘}_1 \ a^{‘}_2 \ a^{‘}_3<br>    \end{pmatrix}<br>$$</p><p>等式两边左乘<br>$$<br>    \begin{pmatrix}<br>    e_1^T &amp; e_2^T &amp; e_3^T<br>    \end{pmatrix}^T<br>$$</p><p>可以得到<br>$$<br>    \begin{pmatrix}<br>    a_1 \ a_2 \ a_3<br>    \end{pmatrix} &#x3D;<br>    \begin{pmatrix}<br>    e_1^Te^{‘}_1 &amp; e_1^Te^{‘}_2 &amp; e_1^Te^{‘}_3 \<br>    e_2^Te^{‘}_1 &amp; e_2^Te^{‘}_2 &amp; e_2^Te^{‘}_3 \<br>    e_3^Te^{‘}_1 &amp; e_3^Te^{‘}_2 &amp; e_3^Te^{‘}_3<br>    \end{pmatrix} \cdot<br>    \begin{pmatrix}<br>    a^{‘}_1 \ a^{‘}_2 \ a^{‘}_3<br>    \end{pmatrix}<br>$$</p><p>现在让我们回忆一下向量内积</p><p><img src="/./PX4%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%80%BE%E8%BD%AC%E5%88%86%E7%A6%BB/%E5%86%85%E7%A7%AF.png" alt="向量内积"></p><p>对于两个向量$e_1$和$e^{‘}_1$，他们的内积$(e^{‘}_1,e_1)&#x3D;e^T_1\cdot e^{‘}_1$,而他们的几何意义可以看成$e^{‘}_1$在$e_1$方向上投影的模再乘$e_1$本身的模，$e_1$又是单位向量，模值为1，所以再这种情况下，$e^T_1\cdot e^{‘}_1$的值就等于$e^{‘}_1$在$e_1$方向上投影的模。</p><p>观察旋转矩阵，可以发现旋转矩阵的每一列都是旋转后的坐标系下的基在原坐标系下的基的投影分量，放到姿态解算这个问题上来，设地理坐标系为北东地（NED）坐标系，各xyz坐标轴上的单位向量为一组基，那么旋转过后的坐标系，也就是机体坐标系，这两个坐标系之间的旋转矩阵的第一列代表机体坐标系下的x轴在地理坐标系xyz轴上各轴的分量，也可以理解为坐标，第二列表示机体坐标系下y轴在地理坐标系xyz轴上各轴的分量，以此类推。</p><h3 id="PX4的倾转分离实现"><a href="#PX4的倾转分离实现" class="headerlink" title="PX4的倾转分离实现"></a>PX4的倾转分离实现</h3><p>根据上面的算法我们知道，在当前机体坐标系转到期望机体坐标系之前，首先要得到一个中间坐标系，这个坐标系将当前的机体坐标系和期望机体坐标系的z轴进行了对齐，但是保留了$yaw$的误差，就像下图所示。</p><p><img src="/./PX4%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%80%BE%E8%BD%AC%E5%88%86%E7%A6%BB/%E5%9D%90%E6%A0%87%E6%97%8B%E8%BD%AC.png" alt="各坐标系一览"></p><p>可以看到，中间坐标系（灰色）和期望坐标系（蓝色）的区别仅仅是z轴的旋转。那么首先就要获得这个中间坐标系。</p><p>在PX4的姿态控制代码中，首先获得了当前姿态和期望姿态的四元数$q^{N}<em>{cur}$和$q^{N}</em>{tar}$<br>然后把当前姿态和期望姿态的z轴向量提取出来，也就是先转换成旋转矩阵，根据上面旋转矩阵得出的结论可以知道，旋转矩阵的第三列就是机体坐标系z轴在地理坐标系下对应的向量坐标。</p><p>得到了两个向量，可以通过这两个向量，构造出旋转四元数$q_{red}$，注意这里的旋转四元数是在地理坐标系下构建的，不能用于链式法则。或者说，这个四元数应该长成这样：</p><p>$$<br>    [q^{cur}_{half}]^N<br>$$</p><p>换句话说，这个四元数是在地理坐标系下看到的当前坐标系与中间坐标系之间的误差。因此要先将其转换到机体坐标系下。根据公式，有</p><p>$$<br>    [q^{cur}<em>{half}]^N&#x3D;(q^{N}</em>{cur}) \cdot q^{cur}<em>{half} \cdot (q^{N}</em>{cur})^{-1}<br>$$</p><p>得到</p><p>$$<br>    q^{cur}<em>{half}&#x3D;(q^{N}</em>{cur})^{-1} \cdot [q^{cur}<em>{half}]^N \cdot (q^{N}</em>{cur})<br>$$</p><p>转到机体坐标系下后就可以用链式法则了。中间坐标系到地理坐标系的旋转四元数为</p><p>$$<br>    q^{N}<em>{half}&#x3D;q^{N}</em>{cur} \cdot q_{half}^{cur} &#x3D; q^{N}<em>{cur} \cdot (q^{N}</em>{cur})^{-1} \cdot [q^{cur}<em>{half}]^N \cdot (q^{N}</em>{cur}) &#x3D; [q^{cur}<em>{half}]^N \cdot (q^{N}</em>{cur})<br>$$</p><p>这样就得到中间坐标系的旋转四元数了。下一步是得到旋转误差$torsion$四元数，这个四元数就是把中间坐标系旋转到目标坐标系，我们把它叫$q_{tar}^{half}$。由链式法则可以得到</p><p>$$<br>  q_{tar}^{half}&#x3D;q_{N}^{half} \cdot q_{tar}^{N} &#x3D; (q_{half}^{N})^{-1} \cdot q_{tar}^{N}<br>$$</p><p>这个四元数就表征了$yaw$误差的大小，根据轴角定义可得，这个四元数正常来说应该长这样：</p><p>$$<br>    q_{tar}^{half}&#x3D;(\cos{\theta&#x2F;2},0,0,\sin{\theta&#x2F;2 \cdot z})<br>$$</p><p>$\theta$就是要转的$yaw$，也就是$yaw$轴误差。我们可以对这个误差进行限幅或者衰减。</p><p>最后，把衰减后的旋转误差四元数和倾斜误差四元数合成，得到新的期望四元数</p><p>$$<br>    q_{tar}^{N} &#x3D; q^{N}<em>{half} \cdot q</em>{tar}^{half}<br>$$</p><p>接下来，获得总的误差四元数</p><p>$$<br>    q_{tar}^{N} &#x3D; q^{N}<em>{cur} \cdot q^{cur}</em>{tar}, \<br>    q_{e} &#x3D; q^{cur}<em>{tar} &#x3D; (q^{N}</em>{cur})^{-1} \cdot q_{tar}^{N}<br>$$</p><p>至此，倾转分离完成。</p><p>PX4相关部分的代码实现：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">matrix::Vector3f AttitudeControl::update(const Quatf &amp;q) const</span><br><span class="line">&#123;</span><br><span class="line">Quatf qd = _attitude_setpoint_q;</span><br><span class="line"></span><br><span class="line">// calculate reduced desired attitude neglecting vehicle&#x27;s yaw to prioritize roll and pitch</span><br><span class="line">const Vector3f e_z = q.dcm_z();</span><br><span class="line">const Vector3f e_z_d = qd.dcm_z();</span><br><span class="line">Quatf qd_red(e_z, e_z_d);</span><br><span class="line"></span><br><span class="line">if (fabsf(qd_red(1)) &gt; (1.f - 1e-5f) || fabsf(qd_red(2)) &gt; (1.f - 1e-5f)) &#123;</span><br><span class="line">// In the infinitesimal corner case where the vehicle and thrust have the completely opposite direction,</span><br><span class="line">// full attitude control anyways generates no yaw input and directly takes the combination of</span><br><span class="line">// roll and pitch leading to the correct desired yaw. Ignoring this case would still be totally safe and stable.</span><br><span class="line">qd_red = qd;</span><br><span class="line"></span><br><span class="line">&#125; else &#123;</span><br><span class="line">// transform rotation from current to desired thrust vector into a world frame reduced desired attitude</span><br><span class="line">qd_red *= q;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// mix full and reduced desired attitude</span><br><span class="line">Quatf q_mix = qd_red.inversed() * qd;</span><br><span class="line">q_mix.canonicalize();</span><br><span class="line">// catch numerical problems with the domain of acosf and asinf</span><br><span class="line">q_mix(0) = math::constrain(q_mix(0), -1.f, 1.f);</span><br><span class="line">q_mix(3) = math::constrain(q_mix(3), -1.f, 1.f);</span><br><span class="line">qd = qd_red * Quatf(cosf(_yaw_w * acosf(q_mix(0))), 0, 0, sinf(_yaw_w * asinf(q_mix(3))));</span><br><span class="line"></span><br><span class="line">// quaternion attitude control law, qe is rotation from q to qd</span><br><span class="line">const Quatf qe = q.inversed() * qd;</span><br><span class="line"></span><br><span class="line">// using sin(alpha/2) scaled rotation axis as attitude error (see quaternion definition by axis angle)</span><br><span class="line">// also taking care of the antipodal unit quaternion ambiguity</span><br><span class="line">const Vector3f eq = 2.f * qe.canonical().imag();</span><br><span class="line"></span><br><span class="line">// calculate angular rates setpoint</span><br><span class="line">matrix::Vector3f rate_setpoint = eq.emult(_proportional_gain);</span><br><span class="line"></span><br><span class="line">// Feed forward the yaw setpoint rate.</span><br><span class="line">// yawspeed_setpoint is the feed forward commanded rotation around the world z-axis,</span><br><span class="line">// but we need to apply it in the body frame (because _rates_sp is expressed in the body frame).</span><br><span class="line">// Therefore we infer the world z-axis (expressed in the body frame) by taking the last column of R.transposed (== q.inversed)</span><br><span class="line">// and multiply it by the yaw setpoint rate (yawspeed_setpoint).</span><br><span class="line">// This yields a vector representing the commanded rotatation around the world z-axis expressed in the body frame</span><br><span class="line">// such that it can be added to the rates setpoint.</span><br><span class="line">if (is_finite(_yawspeed_setpoint)) &#123;</span><br><span class="line">rate_setpoint += q.inversed().dcm_z() * _yawspeed_setpoint;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// limit rates</span><br><span class="line">for (int i = 0; i &lt; 3; i++) &#123;</span><br><span class="line">rate_setpoint(i) = math::constrain(rate_setpoint(i), -_rate_limit(i), _rate_limit(i));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">return rate_setpoint;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="仿真验证"><a href="#仿真验证" class="headerlink" title="仿真验证"></a>仿真验证</h3><p>贴几张网上的博主关于此部分的matlab代码验证。</p><p><img src="/./PX4%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%80%BE%E8%BD%AC%E5%88%86%E7%A6%BB/%E4%BB%BF%E7%9C%9F%E9%AA%8C%E8%AF%81.png" alt="Matlab验证"><br>图中绿色坐标系为当前机体坐标系，红色坐标系为期望机体坐标系<br>黑色虚线为不进行「倾转分离」时的旋转，那么得到的误差，可以让坐标系完全重合。<br>蓝色虚线为「倾转分离」后计算的旋转，只能保证z重合，即roll 和 pitch 没有误差。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>通过倾转分离，有效的对$yaw$轴误差进行了限制，有利于控制器分配更多的能量用于更加关键的$pitch$和$roll$控制，实际上就是把倾斜的控制优先级提高了。</p>]]></content>
    
    
    <summary type="html">&lt;center&gt;终于开始学PX4了&lt;/center&gt;</summary>
    
    
    
    <category term="PX4" scheme="https://erenjaeger-01.github.io/categories/PX4/"/>
    
    
    <category term="PX4" scheme="https://erenjaeger-01.github.io/tags/PX4/"/>
    
  </entry>
  
  <entry>
    <title>RM南航进国赛了</title>
    <link href="https://erenjaeger-01.github.io/2022/06/17/RM%E5%8D%97%E8%88%AA%E8%BF%9B%E5%9B%BD%E8%B5%9B%E4%BA%86/"/>
    <id>https://erenjaeger-01.github.io/2022/06/17/RM%E5%8D%97%E8%88%AA%E8%BF%9B%E5%9B%BD%E8%B5%9B%E4%BA%86/</id>
    <published>2022-06-17T12:06:11.000Z</published>
    <updated>2022-06-17T12:27:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>时至今日，RoboMaster仍然是我本科最大的遗憾。</p><p>18年南理工分区赛，技术不成熟白给</p><p>19年西安分区赛，超级电容白给</p><p>20年疫情，线上举办</p><p>21年考研，没参与</p><p>今年终于打进国赛了，小组赛三场全胜，淘汰赛暴打天大挺进前八。虽然还有很多问题没有处理好，但是比之前真的好太多了。直播看的我无比激动，直播获胜的那一刻我看到许多熟悉的脸在现场激动的欢呼，也想在现场，感受和他们一样的快乐，可惜我早已不在南航，也不会再打比赛了。</p><p>希望学弟学妹们能够不负众望，砥砺前行，南航自从15年参加这个比赛以来，没有一年出过分区赛，今年是创造历史的一年，希望这种精神永远传承下去。</p>]]></content>
    
    
    <summary type="html">&lt;center&gt;创造历史了&lt;/center&gt;</summary>
    
    
    
    <category term="杂记" scheme="https://erenjaeger-01.github.io/categories/%E6%9D%82%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ORB-SLAM1 论文阅读</title>
    <link href="https://erenjaeger-01.github.io/2022/06/15/ORB-SLAM1-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>https://erenjaeger-01.github.io/2022/06/15/ORB-SLAM1-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2022-06-15T13:01:45.000Z</published>
    <updated>2022-07-15T08:32:32.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>这篇论文提出了ORB-SLAM，一个基于特征点的实时单目SLAM系统，可以运行在小型或大型、室内或室外环境。该SLAM系统在剧烈运动时有很好的鲁棒性，允许宽基线的闭环检测和重定位，同时可以全自动的对系统进行初始化。以近年来提出的优秀算法为基础，该系统拥有一个SLAM系统所要求的所有功能：跟踪、建图、重定位和闭环检测。该系统采用了一种优胜劣汰的策略来选择重定位中的地图点和关键帧，这种方法带来了极好的鲁棒性，并且生成了紧凑和可跟踪的地图——当且仅当场景内容改变时才会生长和更新，并且可以一直运作。在测试阶段，作者从最流行的数据集中采取了27个序列对该系统进行了评估，相对于其他单目SLAM方法，ORB-SLAM的表现相当出色，为了便于交流，作者还开源了源代码。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>光束平差法（Bundle Adjustment）是一种广为人知的方法用来精确估计相机定位以及稀疏几何重建，提供了一种强大的匹配网络和良好的初始估计。很长一段时间以来这种方法被认为不能够用到实时应用中，比如视觉SLAM。视觉SLAM的目标是在重建周围环境的同时对相机的轨迹进行估计。如今我们知道要想在一个不大的计算开销中获得精确的结果，一个实时SLAM系统需要提供给BA如下几个方面：</p><ul><li>在被选择的帧（关键帧）的子集中选择对应的场景特征（地图点）观测。</li><li>随着关键帧数量的增加，计算复杂度也随之增加，因此在选择关键帧时应该避免不必要的冗余。</li><li>为了产生准确的结果，需要一个强大的关键帧和地图点的网络配置，这是说，一个良好传播的关键帧观测点集应该有显著的视察和大量的闭环匹配。</li><li>用于非线性优化的关键帧位姿和地图点定位的初始估计。</li><li>一个在探索过程中运行的局部地图，该地图优化的重点是实现可伸缩性。</li><li>能够实行快速的全局优化（比如位姿图）用于实时的闭环操作。</li></ul><p>第一个在实时系统中的BA应用是Mouragon提出的视觉里程计，随后Klein和Murray的开创性SLAM工作，也就是广为人知的平行跟踪和建图（PTAM）。这个算法虽然局限性小尺度运算，但是提供了简单而有效的方法用来进行关键帧选择、特征匹配、三角化、对于每一帧的相机定位和在跟踪失败之后的重定位。遗憾的是尺度因素极大的局限了该方法：缺少闭环检测和恰当的遮挡处理，对于重定位的观点不变性低并且需要人为引导地图生成。<br>在ORB-SLAM中我们以PTAM的主要思想为基础，利用G´alvez-L´opez和Tard´os的位置识别工作，Strasdat的尺度感知闭环技术以及对大规模操作的共视信息利用，从头设计了一种新颖的单目SLAM系统，其主要贡献是：</p><ul><li>对所有的任务都是用同样的特征：跟踪、建图、重定位和闭环检测。这让我们的系统更加的有效、简单和可靠。我们使用ORB特征点，该特征点可以在没有GPU的条件下实时运行，在改变的视角和光照条件下都提供了很好的稳定性。</li><li>在大型环境下的实时操作。这得益于共视图的使用，跟踪和建图时重点关注局部的共视区域，和全局地图的大小独立。</li><li>基于位姿图的优化（我们管这个叫本质图）可以实时闭环。本质图是通过生成树建立的，该生成树基于系统、闭环连接和共视图中较为显著的边组成。</li><li>对视角和光照变化不敏感的实时相机重定位.这使得从跟踪失败的情况下恢复位姿和增强地图的复用性称为可能。</li><li>一个新的基于模型的自动且鲁棒的初始化过程。该初始化过程可以在平面或非平面的场景中生成初始地图。</li><li>一种优胜劣汰的策略对地图点和关键帧进行选择，遵从宽进严出的准则，这种方法增加了跟踪的鲁棒性，并且增强了长生命周期的操作因为冗余关键帧被剔除了。</li></ul><p>我们对室内和室外环境的流行公共数据集进行了广泛的评估，包括手持场景、汽车驾驶场景和机器人场景中的图片序列。值得一提的是，我们用该方法实现了比现有技术更好的相机定位精度，该方法直接对像素的灰度进行优化，而不是特征重投影误差。我们在IX-B节中讨论了基于特征匹配的方法比直接法更加准确的原因。<br>闭环检测和重定位的方法在我们之前的工作中就已经提出，该系统的初步版本在论文【12】中已经给出。我们正家里初始化和本质图，并对所有的方法进行了完善，我们还详细描述了所有构建模块，并进行了详细的实验验证。<br>据我们所知，这是单目SLAM最完整，最可靠的解决方案，为了社区的利益，我们开源了源代码。演示视频和代码可以在我们的项目网页中找到。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="位置识别"><a href="#位置识别" class="headerlink" title="位置识别"></a>位置识别</h3><p>在Williams的研究工作中，对比了几种位置识别的方法并且基于表现得出了结论，那就是图像到图像的匹配在大型环境中的规模更好，相较于图像到地图的匹配以及地图到地图的匹配。在这些方法的表现中，词袋技术，例如概率方法FAB-MAP，由于其高效率排在前面。DBoW2第一次使用由二进制单词组成的词袋，其中二进制单词由BRIEF描述子和非常高效的FAST特征检测组成。这减少了超过一个数量级的特征提取时间消耗，相较于SURF和SIFT特征检测，词袋方法快了太多。尽管这个系统被证实是非常高效和鲁棒的，但由于BRIEF没有旋转和尺度不变性，限制了系统在平面上的跟踪以及相似视角下的闭环检测。在我们先前的工作中，我们提出了一种基于DBoW2和ORB的词袋位置检测方法。ORB是一种二进制特征并且具有尺度和旋转不变性（在一个确定的范围内），带来了非常快速地检测以及很好的变视角情况下的鲁棒性。我们在四种不同的数据集中证明了这种检测方法的高召回率和鲁棒性，从10K图像的数据集中循环检索图像的时间小于39ms（包括特征提取的时间）。在这项工作中我们使用了一种改进的版本，使用共视信息并且返回几个可能的假设而不是只返回一个最佳匹配。</p><h3 id="地图初始化"><a href="#地图初始化" class="headerlink" title="地图初始化"></a>地图初始化</h3><p>单目SLAM需要有一个创建初始地图的过程，因为单目图像中并不能恢复深度信息。一个解决这种问题的方法是最初跟踪一个已知的结构，在基于滤波的方法中，点的初始化可以通过逆深度参数化的方式在高度不确定的深度情况下完成，这种方法有希望在之后的过程中将地图点收敛到他们真实的位置。Engel等人最近的半稠密工作中，采用了一种类似的方法将像素的深度信息初始化为一个高方差的随机值。<br>来自两个视图的初始化方法要么假设局部场景是平面的，然后利用Faugeras等人的方法通过单应矩阵恢复相关的相机位姿，或者计算本质矩阵，在非平面或平面的场景中都可以，利用Nister提出的五点法，该方法需要处理多个解决方案。两种重建方法在低视差的情况时都没有很好的约束并且如果视景中的地图点都特别靠近相机的中心时会产生双重模糊解。另一方面如果一个非平面的场景出现视差时，可以通过八点法计算出一个唯一的基础矩阵，并且相关的相机位姿可以在没有模糊的条件下恢复。<br>我们在第四节展示了一个新的基于模型在平面场景下通过单应矩阵以及非平面场景下通过基础矩阵两种方法自动选择的方案。Torr等人提出了一种基于统计的方法去选择模型。在类似的原理下我们开发了一种启发式初始化算法，该算法将在接近退化条件时选择基础矩阵的风险考虑其中（平面、接近平面和低视差），当风险过高时我们选择单应矩阵。在平面情况下，从安全性考虑，如果最终的解有双重模糊性，我们就不初始化，因为可能选择一个错误的解。我们延迟初始化直到该方法在显著视差的情况下计算出一个唯一的解。</p>]]></content>
    
    
    <summary type="html">&lt;center&gt;虚晃一枪&lt;/center&gt;</summary>
    
    
    
    <category term="SLAM" scheme="https://erenjaeger-01.github.io/categories/SLAM/"/>
    
    
    <category term="SLAM" scheme="https://erenjaeger-01.github.io/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>ORB-SLAM学习（四）：特征匹配</title>
    <link href="https://erenjaeger-01.github.io/2022/05/23/ORB-SLAM%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%8C%B9%E9%85%8D/"/>
    <id>https://erenjaeger-01.github.io/2022/05/23/ORB-SLAM%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%8C%B9%E9%85%8D/</id>
    <published>2022-05-23T11:51:08.000Z</published>
    <updated>2022-05-23T12:07:46.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="快速搜索候选匹配特征点"><a href="#快速搜索候选匹配特征点" class="headerlink" title="快速搜索候选匹配特征点"></a>快速搜索候选匹配特征点</h2><p>之前进行特征点提取的时候对图像进行了网格分割，特征点就存储在对应的网格中，特征匹配的时候就通过这些网格减小搜索范围，提高搜索效率。</p><p>特征提取步骤如下：</p><ul><li>首先，对参考帧的每一个特征点，在当前帧中进行寻找，怎么找呢？以这个特征点位置为圆心，设置一个搜索半径，就在这个圆内寻找可能的特征点，找出来一堆（不止一个）。</li><li>在找出来的这一堆特征点里面寻找和参考特征点最匹配的特征点（比较描述子距离），找出来最优的和次优的，同时要确保找出来的最佳特征点描述子之间的距离小于设定的最小阈值。</li><li>找完之后还要进行角度筛查，这一步是通过建立旋转角度直方图实现的。</li></ul><h2 id="建立直方图对角度不一致的特征点进行过滤"><a href="#建立直方图对角度不一致的特征点进行过滤" class="headerlink" title="建立直方图对角度不一致的特征点进行过滤"></a>建立直方图对角度不一致的特征点进行过滤</h2><p>建立了一个30栏的直方图，存储360°的角度细分，每一列包含12°。</p><ul><li>对找出来的符合要求的特征点，用参考帧特征点的角度减去当前帧特征点的角度，得到的结果存入直方图中（这样大部分的特征点对会进入同一列中，代表了旋转的角度）。</li><li>选出频率最高的前三列，这里面的特征点对就是匹配好了的，剩下的特征点对就不要了，被认为是误匹配。</li></ul>]]></content>
    
    
    <summary type="html">&lt;center&gt;建立了当前帧图像和上一帧图像之间的对应关系&lt;/center&gt;</summary>
    
    
    
    <category term="SLAM" scheme="https://erenjaeger-01.github.io/categories/SLAM/"/>
    
    
    <category term="SLAM" scheme="https://erenjaeger-01.github.io/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>ORB-SLAM学习（三）：地图点、关键帧和图结构</title>
    <link href="https://erenjaeger-01.github.io/2022/05/13/ORB-SLAM%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%9C%B0%E5%9B%BE%E7%82%B9%E3%80%81%E5%85%B3%E9%94%AE%E5%B8%A7%E5%92%8C%E5%9B%BE%E7%BB%93%E6%9E%84/"/>
    <id>https://erenjaeger-01.github.io/2022/05/13/ORB-SLAM%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%9C%B0%E5%9B%BE%E7%82%B9%E3%80%81%E5%85%B3%E9%94%AE%E5%B8%A7%E5%92%8C%E5%9B%BE%E7%BB%93%E6%9E%84/</id>
    <published>2022-05-13T12:40:58.000Z</published>
    <updated>2022-05-14T01:37:50.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="地图点"><a href="#地图点" class="headerlink" title="地图点"></a>地图点</h2><p>地图点就是真实世界中的点，来自真实世界的三维物体，有唯一的ID，不同图像帧里面的特征点可能对应三维空间中的同一个地图点。</p><h3 id="地图点生成"><a href="#地图点生成" class="headerlink" title="地图点生成"></a>地图点生成</h3><p>地图点生成在ORB-SLAM2中主要存在于下面几个地方：</p><ul><li>单目初始化时前两帧会生成一部分地图点，双目初始化通过左右目匹配第一帧就可以生成地图点，RGB-D相机通过测量深度也可以在第一帧生成部分地图点；</li><li>局部建图线程里面，共视关键帧之间通过*LocalMapping::CreateNewMapPoints()*函数生成地图点；</li><li>跟踪线程里面，*Tracking::UpdateLastFrame()<em>和</em>Tracking::CreateNewKeyFrame()*函数中为双目和RGB-D相机生成新的<strong>临时</strong>地图点，单目相机不生成地图点。</li></ul><h3 id="计算地图点最具代表性的描述子"><a href="#计算地图点最具代表性的描述子" class="headerlink" title="计算地图点最具代表性的描述子"></a>计算地图点最具代表性的描述子</h3><p>由于一个地图点会被多个相机图像观测到，因此在插入关键帧后，需要判断是否更新当前点的描述子。</p><p><img src="/./ORB-SLAM%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%9C%B0%E5%9B%BE%E7%82%B9%E3%80%81%E5%85%B3%E9%94%AE%E5%B8%A7%E5%92%8C%E5%9B%BE%E7%BB%93%E6%9E%84/%E4%BB%A3%E8%A1%A8%E6%8F%8F%E8%BF%B0%E5%AD%90.png" alt="找出合适的描述子描述该地图点"></p><p>那么代表性描述子如何找出来呢？具体的操作为首先获得当前点的所有描述子，然后计算描述子之间的两两距离。在ORB-SLAM2中，最好的描述子与其他的描述子之间应该具有最小的距离中值。</p><p><img src="/./ORB-SLAM%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%9C%B0%E5%9B%BE%E7%82%B9%E3%80%81%E5%85%B3%E9%94%AE%E5%B8%A7%E5%92%8C%E5%9B%BE%E7%BB%93%E6%9E%84/%E8%B7%9D%E7%A6%BB%E4%B8%AD%E5%80%BC.png" alt="距离中值"></p><h4 id="计算描述子距离"><a href="#计算描述子距离" class="headerlink" title="计算描述子距离"></a>计算描述子距离</h4><p>在函数*ORBmatcher::DescriptorDistance()*中，通过汉明距离进行计算</p><p><strong>汉明距离</strong>：汉明距离是一个概念，它表示两个（相同长度）字符串对应位置的不同字符的数量，我们以d（x,y）表示两个字x,y之间的汉明距离。对两个字符串进行异或运算，并统计结果为1的个数，那么这个数就是汉明距离。</p><p>举个例子，1011101和1001001之间的汉明距离是2，因为第三位和第六位不同。</p><p>由于BRIEF描述子是一堆二进制串，所以通过汉明距离计算非常方便</p><p>ORB-SLAM2中计算BRIEF描述子汉明距离的代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">//pa,pb是两BRIEF描述子，每个描述子8*32=256位</span><br><span class="line"></span><br><span class="line">for(int i=0; i&lt;8; i++, pa++, pb++)</span><br><span class="line">    &#123;</span><br><span class="line">        unsigned  int v = *pa ^ *pb;    //异或操作，相等为0，不等为1     </span><br><span class="line">        v = v - ((v &gt;&gt; 1) &amp; 0x55555555);</span><br><span class="line">        v = (v &amp; 0x33333333) + ((v &gt;&gt; 2) &amp; 0x33333333);</span><br><span class="line">        dist += (((v + (v &gt;&gt; 4)) &amp; 0xF0F0F0F) * 0x1010101) &gt;&gt; 24;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>对这段代码进行解读，这段代码实际上使用了一个泛用的算法，叫SWAR算法，用于计算二进制数中1的个数。整体思路就是先将相邻2位的1的数量计算出来，结果存放在这2位。然后将相邻4位的结果相加，结果存放在这4位，将相邻8位的结果相加，结果存放在这8位。最后计算整体1的数量，记录在高8位，然后通过右移运算，将结果放到低8位，得到最终结果。</p><p>SWAR算法通过先将数字进行最细力粒度的拆分，然后每一步都对上一次的计算结果进行整合，最终得到整体结果。由于这里处理的是32位数，所以这个算法比遍历算法快32倍，也不需要消耗额外内存空间。</p><h3 id="地图点平均观测方向、观测距离范围"><a href="#地图点平均观测方向、观测距离范围" class="headerlink" title="地图点平均观测方向、观测距离范围"></a>地图点平均观测方向、观测距离范围</h3><p><img src="/./ORB-SLAM%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%9C%B0%E5%9B%BE%E7%82%B9%E3%80%81%E5%85%B3%E9%94%AE%E5%B8%A7%E5%92%8C%E5%9B%BE%E7%BB%93%E6%9E%84/%E5%9C%B0%E5%9B%BE%E7%82%B9%E6%9C%9D%E5%90%91.png" alt="地图点法线朝向的计算"></p><p>对于观测到某一地图点的全部关键帧，对该点的观测方向归一化为单位向量，然后进行求和得到该地图点的朝向。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">void MapPoint::UpdateNormalAndDepth()</span><br><span class="line">&#123;</span><br><span class="line">    // Step 1 获得观测到该地图点的所有关键帧、坐标等信息</span><br><span class="line">    map&lt;KeyFrame*,size_t&gt; observations;</span><br><span class="line">    KeyFrame* pRefKF;</span><br><span class="line">    cv::Mat Pos;</span><br><span class="line">    &#123;</span><br><span class="line">        unique_lock&lt;mutex&gt; lock1(mMutexFeatures);</span><br><span class="line">        unique_lock&lt;mutex&gt; lock2(mMutexPos);</span><br><span class="line">        if(mbBad)</span><br><span class="line">            return;</span><br><span class="line"></span><br><span class="line">        observations=mObservations; // 获得观测到该地图点的所有关键帧</span><br><span class="line">        pRefKF=mpRefKF;             // 观测到该点的参考关键帧（第一次创建时的关键帧）</span><br><span class="line">        Pos = mWorldPos.clone();    // 地图点在世界坐标系中的位置</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if(observations.empty())</span><br><span class="line">        return;</span><br><span class="line"></span><br><span class="line">    // Step 2 计算该地图点的平均观测方向</span><br><span class="line">    // 能观测到该地图点的所有关键帧，对该点的观测方向归一化为单位向量，然后进行求和得到该地图点的朝向</span><br><span class="line">    // 初始值为0向量，累加为归一化向量，最后除以总数n</span><br><span class="line">    cv::Mat normal = cv::Mat::zeros(3,1,CV_32F);</span><br><span class="line">    int n=0;</span><br><span class="line">    for(map&lt;KeyFrame*,size_t&gt;::iterator mit=observations.begin(), mend=observations.end(); mit!=mend; mit++)</span><br><span class="line">    &#123;</span><br><span class="line">        KeyFrame* pKF = mit-&gt;first;</span><br><span class="line">        cv::Mat Owi = pKF-&gt;GetCameraCenter();</span><br><span class="line">        // 获得地图点和观测到它关键帧的向量并归一化</span><br><span class="line">        cv::Mat normali = mWorldPos - Owi;</span><br><span class="line">        normal = normal + normali/cv::norm(normali);                       </span><br><span class="line">        n++;</span><br><span class="line">    &#125; </span><br><span class="line"></span><br><span class="line">    cv::Mat PC = Pos - pRefKF-&gt;GetCameraCenter();                           // 参考关键帧相机指向地图点的向量（在世界坐标系下的表示）</span><br><span class="line">    const float dist = cv::norm(PC);                                        // 该点到参考关键帧相机的距离</span><br><span class="line">    const int level = pRefKF-&gt;mvKeysUn[observations[pRefKF]].octave;        // 观测到该地图点的当前帧的特征点在金字塔的第几层</span><br><span class="line">    const float levelScaleFactor =  pRefKF-&gt;mvScaleFactors[level];          // 当前金字塔层对应的尺度因子，scale^n，scale=1.2，n为层数</span><br><span class="line">    const int nLevels = pRefKF-&gt;mnScaleLevels;                              // 金字塔总层数，默认为8</span><br><span class="line"></span><br><span class="line">    &#123;</span><br><span class="line">        unique_lock&lt;mutex&gt; lock3(mMutexPos);</span><br><span class="line">        // 使用方法见PredictScale函数前的注释</span><br><span class="line">        mfMaxDistance = dist*levelScaleFactor;                              // 观测到该点的距离上限</span><br><span class="line">        mfMinDistance = mfMaxDistance/pRefKF-&gt;mvScaleFactors[nLevels-1];    // 观测到该点的距离下限</span><br><span class="line">        mNormalVector = normal/n;                                           // 获得地图点平均的观测方向</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="关键帧"><a href="#关键帧" class="headerlink" title="关键帧"></a>关键帧</h2><p>关键帧就是几帧图像里面比较有代表性的那一帧，可以大概的把它的重要程度比作一幅图像中的特征点。</p><p><img src="/./ORB-SLAM%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%9C%B0%E5%9B%BE%E7%82%B9%E3%80%81%E5%85%B3%E9%94%AE%E5%B8%A7%E5%92%8C%E5%9B%BE%E7%BB%93%E6%9E%84/%E5%85%B3%E9%94%AE%E5%B8%A7.png" alt="ORB-SLAM中的关键帧"></p><h3 id="关键帧的好处"><a href="#关键帧的好处" class="headerlink" title="关键帧的好处"></a>关键帧的好处</h3><ul><li>相邻帧之间的信息冗余度很高，通过关键帧可以降低信息冗余度。比如相机放在原地不动，普通帧一直在累加，但是关键帧始终不变；</li><li>关键帧是普通帧滤波和优化的结果，可以增加定位的准确性；</li><li>关键帧的主要作用是面向后端的算力和精度的折中，使得有限的资源能够用在刀刃上。就好像现在还处于社会主义初级阶段，不能够使所有人都富起来，只好让一部分人先富起来，先富带动后富，最后等生产力极大发展（算力爆炸）后，才能实现所有人一视同仁。</li></ul><h3 id="如何选择关键帧"><a href="#如何选择关键帧" class="headerlink" title="如何选择关键帧"></a>如何选择关键帧</h3><p>目前来说，主要的指标主要有下面这些：</p><ul><li>时间尺度，距离上一关键帧之间的帧数不能太少，但是搁多少帧选择关键帧是个问题；</li><li>运动尺度，计算运动累积的多少，每移动一定的距离就选一个关键帧，但是这样的话如果对着一个物体来回扫就会出现大量相同关键帧；</li><li>根据共视特征点的数量，记录下当前视角下的特征点的数量或者比例，当相机离开场景时才新建关键帧，缺点是数据结构和逻辑比较复杂。</li></ul><p>在ORB-SLAM2中，后期的局部建图和全局BA都是只用关键帧来操作了，在跟踪线程中选择关键帧，这时候的选择标准还比较宽松，在局部建图线程，会根据共视冗余度对关键帧进行剔除，剩下这些才是真正用到的关键帧。</p><h2 id="共视图"><a href="#共视图" class="headerlink" title="共视图"></a>共视图</h2><p>共视图是无向加权图，每个节点都是关键帧，如果关键帧之间满足一定的共视关系（在ORB-SLAM2中的标准是至少有15个共视地图点），就连成一条边，这条边的权重就是共视地图点的数目。</p><p><img src="/./ORB-SLAM%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%9C%B0%E5%9B%BE%E7%82%B9%E3%80%81%E5%85%B3%E9%94%AE%E5%B8%A7%E5%92%8C%E5%9B%BE%E7%BB%93%E6%9E%84/%E5%85%B1%E8%A7%86%E5%9B%BE.png" alt="共视图"></p><h3 id="共视图的作用"><a href="#共视图的作用" class="headerlink" title="共视图的作用"></a>共视图的作用</h3><ul><li>跟踪局部地图，扩大搜索范围；</li><li>关键帧之间新建地图点；</li><li>闭环检测以及重定位检测；</li><li>优化。</li></ul><h2 id="本质图"><a href="#本质图" class="headerlink" title="本质图"></a>本质图</h2><p>共视图比较稠密，本质图比共视图稀疏很多，关键帧作为节点，但是连接边更少，只保留了联系紧密的边。在本质图中，主要包含了</p><ul><li>生成树之间的连接关系；</li><li>形成闭环的连接关系；</li><li>共视关系非常好的连接关系。</li></ul><p><img src="/./ORB-SLAM%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%9C%B0%E5%9B%BE%E7%82%B9%E3%80%81%E5%85%B3%E9%94%AE%E5%B8%A7%E5%92%8C%E5%9B%BE%E7%BB%93%E6%9E%84/%E6%9C%AC%E8%B4%A8%E5%9B%BE.png" alt="本质图"></p><h2 id="生成树"><a href="#生成树" class="headerlink" title="生成树"></a>生成树</h2><p>生成树由子关键帧和父关键帧构成，也就是共视关系最高的帧，只要子关键帧存在父关键帧就生成连接关系，不然就被孤立，如下图中黄色的线。</p><p><img src="/./ORB-SLAM%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%9C%B0%E5%9B%BE%E7%82%B9%E3%80%81%E5%85%B3%E9%94%AE%E5%B8%A7%E5%92%8C%E5%9B%BE%E7%BB%93%E6%9E%84/%E7%94%9F%E6%88%90%E6%A0%91.png" alt="生成树"></p>]]></content>
    
    
    <summary type="html">&lt;center&gt;每个人都只有一个脑袋&lt;/center&gt;</summary>
    
    
    
    <category term="SLAM" scheme="https://erenjaeger-01.github.io/categories/SLAM/"/>
    
    
    <category term="SLAM" scheme="https://erenjaeger-01.github.io/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>数学基础（二）：卡方检验</title>
    <link href="https://erenjaeger-01.github.io/2022/05/09/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C/"/>
    <id>https://erenjaeger-01.github.io/2022/05/09/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C/</id>
    <published>2022-05-09T13:15:00.000Z</published>
    <updated>2022-05-10T03:09:32.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="卡方检验"><a href="#卡方检验" class="headerlink" title="卡方检验"></a>卡方检验</h3><p>卡方检验通过一个检验统计量来比较期望结果和实际结果之间的差别，然后得出观察频数极值的发生概率。<br>计算统计量步骤： （期望频数总和与观察频数总和相等）</p><ul><li>表里填写相应的观察频数和期望频数</li><li>利用卡方公式计算检验统计量：</li></ul><p>$$<br>   \chi^2&#x3D;\sum\frac{(O-E)^2}{E}<br>$$</p><p>下面进行相关说明：</p><ul><li><p>O代表观察到的频数，也就是实际发生的频数。E代表期望频数。</p></li><li><p>检验统计量$\chi^2$意义：O与E之间差值越小，检验统计量越小。以E为除数，令差值与期望频数成比例。</p></li><li><p>卡方检验的标准：如果统计量值很小，说明观察频数和期望频数之间的差别不显著，统计量越大，差别越显著。</p></li><li><p>自由度：自由度用于计算检验统计量的独立变量数目，查表用的</p><p>自由度计算：对于单行或单列，自由度&#x3D;组数-限制数；对于表格类，自由度&#x3D;（行数-1）*（列数-1）</p></li></ul><p>卡方分布表如下所示</p><p><img src="/./%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C/%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83%E8%A1%A8.png" alt="卡方分布表"></p>]]></content>
    
    
    <summary type="html">&lt;center&gt;买彩票&lt;/center&gt;</summary>
    
    
    
    <category term="Math" scheme="https://erenjaeger-01.github.io/categories/Math/"/>
    
    
    <category term="Math" scheme="https://erenjaeger-01.github.io/tags/Math/"/>
    
  </entry>
  
  <entry>
    <title>数学基础（一）：奇异值分解</title>
    <link href="https://erenjaeger-01.github.io/2022/05/09/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/"/>
    <id>https://erenjaeger-01.github.io/2022/05/09/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/</id>
    <published>2022-05-09T13:13:49.000Z</published>
    <updated>2022-05-10T03:09:14.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="奇异值分解SVD"><a href="#奇异值分解SVD" class="headerlink" title="奇异值分解SVD"></a>奇异值分解SVD</h3><p>SVD分解定义</p><p>$$<br>   A&#x3D;U\sum{}V^T<br>$$</p><p>其中，A是$m\times n$的矩阵，根据奇异值分解后，U是$m\times m$的矩阵，$\sum{}$是一个$m\times n$的矩阵，除主对角线上的元素以外全部为0，主对角线上的每个元素都称为奇异值，V是一个$n\times n$的矩阵，U和V都是酉矩阵，即满足</p><p>$$<br>U^TU&#x3D;I,V^TV&#x3D;I<br>$$</p><p>可以用下图表示奇异值分解：</p><p><img src="/./%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3.png" alt="奇异值分解"></p><p>对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。<br>也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。也就是说：</p><p>$$<br>   A_{m\times n}&#x3D;U_{m\times n}\sum{}<em>{m\times n}V^T</em>{n\times n}\approx U_{m\times k}\sum{}<em>{k\times k}V^T</em>{k\times n}<br>$$</p><p>其中k比n小很多，也就是一个大矩阵A可以用三个小矩阵$U_{m\times k},\sum{}<em>{k\times k},V^T</em>{k\times n}$来表示。</p><p>SVD求解最小二乘问题：</p><p>$$<br>   min\begin{Vmatrix}<br>      Ax-b<br>   \end{Vmatrix}^2,A\in R^{m\times n},x\in R^{n},b\in R^{m}<br>$$</p><p>m个方程求n个未知数，有三种情况：</p><ul><li>$m&#x3D;n$且A非奇异，则有唯一解（线性代数学到的）</li><li>$m&gt;n$，约束的个数超过未知数个数，称为超定问题</li><li>$m&lt;n$，约束的个数小于未知数个数，称为负定&#x2F;欠定问题</li></ul><p>通常我们遇到的都是超定问题，此时是没有解的，从而转向最小二乘问题：</p><p>$$<br>   J(x)&#x3D;min\begin{Vmatrix}<br>      Ax-b<br>   \end{Vmatrix}^2<br>$$</p><p>懒得敲公式了。上个链接<br><a href="https://zhuanlan.zhihu.com/p/436753966">奇异值分解与最小二乘法</a></p><p>总结一下结论，对于$Ax&#x3D;0$</p><p>SVD分解之后得到的V最右侧的列向量(也就是$V^T最下方的行向量$)就是一般性齐次线性方程组$Ax&#x3D;0$的解。</p>]]></content>
    
    
    <summary type="html">&lt;center&gt;这章比较懒&lt;/center&gt;</summary>
    
    
    
    <category term="Math" scheme="https://erenjaeger-01.github.io/categories/Math/"/>
    
    
    <category term="Math" scheme="https://erenjaeger-01.github.io/tags/Math/"/>
    
  </entry>
  
  <entry>
    <title>计算机视觉系列（五）：基础矩阵和本质矩阵</title>
    <link href="https://erenjaeger-01.github.io/2022/05/09/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A9%E9%98%B5%E5%92%8C%E6%9C%AC%E8%B4%A8%E7%9F%A9%E9%98%B5/"/>
    <id>https://erenjaeger-01.github.io/2022/05/09/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A9%E9%98%B5%E5%92%8C%E6%9C%AC%E8%B4%A8%E7%9F%A9%E9%98%B5/</id>
    <published>2022-05-09T13:11:10.000Z</published>
    <updated>2022-05-10T03:10:18.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="对极几何"><a href="#对极几何" class="headerlink" title="对极几何"></a>对极几何</h3><p>先放一张图<br><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A9%E9%98%B5%E5%92%8C%E6%9C%AC%E8%B4%A8%E7%9F%A9%E9%98%B5/%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="对极约束示意图"></p><p>设第一帧到第二帧的运动为$R,t$,两个相机中心分别为$O_1,O_2$。$p_1$是$I_1$中的特征点，对应到$I_2$中的特征点为$p_2$。下面通过一些术语来描述他们之间的几何关系。首先，连线$\vec{O_1 p_1}$和连线$\vec{O_2 p_2}$在三维空间中相交于$P$，这是$O_1,O_2,P$可以确定一个平面，称为极平面，$O_1,O_2$连线与像平面$I_1,I_2$的交点分别为$e_1,e_2$，$e_1,e_2$被称为极点，$O_1O_2$被称为基线，我们称极平面与两个像平面$I_1,I_2$之间的相交线$l_1,l_2$为极线。</p><h3 id="基础矩阵"><a href="#基础矩阵" class="headerlink" title="基础矩阵"></a>基础矩阵</h3><p>对于特征点对$p_1,p_2$，用基础矩阵$F_{21}$描述特征点对之间的转换关系</p><p>$$<br>   p_2^TF_{21}p_1&#x3D;0<br>$$</p><p>写成矩阵形式，有</p><p>$$<br>    \begin{bmatrix} v_2&amp;u_2&amp;1 \end{bmatrix}<br>    \begin{bmatrix} f_1&amp;f_2&amp;f_3\f_4&amp;f_5&amp;f_6\f_7&amp;f_8&amp;f_9 \end{bmatrix}<br>    \begin{bmatrix} u_1\v_1\1 \end{bmatrix}&#x3D;0<br>$$</p><p>展开得到</p><p>$$<br>    \begin{bmatrix} u_1<em>u_2&amp;v_1</em>u_2&amp;u_2&amp;u_1<em>v_2&amp;v_1</em>v_2&amp;v_2&amp;u_1&amp;v_1&amp;1 \end{bmatrix}<br>    \begin{bmatrix} f_1\f_2\f_3\f_4\f_5\f_6\f_7\f_8\f_9 \end{bmatrix}&#x3D;0<br>$$</p><p>这样，每对点提供一个约束方程，基础矩阵共有9个元素，7个自由度，且秩为2，所以8对点提供8个约束方程就可以求解F。</p><h3 id="本质矩阵"><a href="#本质矩阵" class="headerlink" title="本质矩阵"></a>本质矩阵</h3><p>本质矩阵E和基础矩阵F差了个相机内参矩阵，恢复位姿的时候一般通过本质矩阵恢复，这样可以屏蔽因为相机内参造成的影响。本质矩阵和基础矩阵的关系为</p><p>$$<br>   E&#x3D;K^TFK , E&#x3D;t^{\land}R<br>$$</p><p>从本质矩阵恢复相机位姿$R，t$</p><p>对E进行奇异值分解（SVD），对于任意一个E，存在两个可能的$R，t$与其对应，又因为-E和E等价，所以对任意一个t取负号，得到的结果是一样的，因此通过本质矩阵E分解到$R，t$时，一共存在4个可能的解。幸运的是，正确的解是唯一的，只有第一个解的空间点P在两个相机中都具有正的深度。</p><p>$$<br>   t_1^{\land}&#x3D;UR_Z(\frac{\pi}{2})DU^T,R_1&#x3D;UR^T_Z(\frac{\pi}{2})V^T<br>$$<br>$$<br>   t_2^{\land}&#x3D;UR_Z(-\frac{\pi}{2})DU^T,R_2&#x3D;UR^T_Z(-\frac{\pi}{2})V^T<br>$$</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A9%E9%98%B5%E5%92%8C%E6%9C%AC%E8%B4%A8%E7%9F%A9%E9%98%B5/%E6%9C%AC%E8%B4%A8%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A31.jpg" alt="本质矩阵分解得到的四组解"></p><h3 id="三角测量（三角化）"><a href="#三角测量（三角化）" class="headerlink" title="三角测量（三角化）"></a>三角测量（三角化）</h3><p>已知一对匹配好的特征点$x_1,x_2$，投影矩阵$P_1,P_2$分别将同一个空间点X投影到两幅图中的$x_1,x_2$。</p><p>描述为：</p><p>$$<br>   x_1&#x3D;\lambda P_1X<br>$$<br>$$<br>   x_2&#x3D;\lambda P_2X<br>$$</p><p>对于每一个表达式可以用通用方程来描述：<br>$$<br>    \begin{bmatrix} x\y\1 \end{bmatrix}&#x3D;\lambda<br>    \begin{bmatrix} p_1&amp;p_2&amp;p_3&amp;p_4\p_5&amp;p_6&amp;p_7&amp;p_8\p_9&amp;p_10&amp;p_11&amp;p_12 \end{bmatrix}<br>    \begin{bmatrix} X\Y\Z\1 \end{bmatrix}<br>$$</p><p>简记为<br>$$<br>    \begin{bmatrix} x\y\1 \end{bmatrix}&#x3D;\lambda<br>    \begin{bmatrix} -&amp;P_0&amp;-\-&amp;P_1&amp;-\-&amp;P_2&amp;-\end{bmatrix}<br>    \begin{bmatrix} X\Y\Z\1 \end{bmatrix}<br>$$</p><p>两边叉乘x，有<br>$$<br>    \begin{bmatrix} 0&amp;-1&amp;y\1&amp;0&amp;-x\-y&amp;x&amp;0 \end{bmatrix}<br>    \begin{bmatrix} -&amp;P_0&amp;-\-&amp;P_1&amp;-\-&amp;P_2&amp;-\end{bmatrix}<br>    \begin{bmatrix} X\Y\Z\1 \end{bmatrix} &#x3D;<br>    \begin{bmatrix} 0\0\0 \end{bmatrix}<br>$$</p><p>一个点就有</p><p>$$<br>    \begin{bmatrix} yP_2-P_1\P_0-xP_2\xP_1-yP_0 \end{bmatrix}<br>    X &#x3D;<br>    \begin{bmatrix} 0\0\0 \end{bmatrix}<br>$$</p><p>秩为2，取前两行就够了</p><p>一对点的情况如下：</p><p>$$<br>    \begin{bmatrix} y_1P_{12}-P_{11}\P_{10}-x_1P_{12}\y_2P_{22}-P_{21}\P_{20}-x_2P_{22} \end{bmatrix}<br>    X &#x3D;<br>    \begin{bmatrix} 0\0\0 \end{bmatrix}<br>$$</p><p>变成最小二乘问题，用奇异值分解即可求解X</p><h3 id="各向同性归一化（八点法）"><a href="#各向同性归一化（八点法）" class="headerlink" title="各向同性归一化（八点法）"></a>各向同性归一化（八点法）</h3><p>利用8点法求基础矩阵不稳定的一个主要原因就是原始的图像像点坐标组成的系数矩阵A不好造成的，而造成A不好的原因是像点的齐次坐标各个分量的数量级相差太大。基于这个原因，在应用8点法求基础矩阵之前，先对像点坐标进行归一化处理，即对原始的图像坐标做同向性变换，这样就可以减少噪声的干扰，大大的提高8点法的精度。</p><p>预先对图像坐标进行归一化有以下好处：</p><ul><li>能够提高运算结果的精度</li><li>利用归一化处理后的图像坐标，对任何尺度缩放和原点的选择是不变的。归一化步骤预先为图像坐标选择了一个标准的坐标系中，消除了坐标变换对结果的影响。</li></ul><p>归一化操作分两步进行，首先对每幅图像中的坐标进行平移（每幅图像的平移不同）使图像中匹配的点组成的点集的形心（Centroid）移动到原点；接着对坐标系进行缩放使得点$p&#x3D;(x,y,w)^T$中的各个分量总体上有一样的平均值，各个坐标轴的缩放相同的，最后选择合适的缩放因子使点p到原点的平均距离是$\sqrt{2}$。 概括起来变换过程如下：</p><ul><li>对点进行平移使其形心位于原点。</li><li>对点进行缩放，使它们到原点的平均距离为$\sqrt{2}$</li><li>对两幅图像独立进行上述变换</li></ul><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A9%E9%98%B5%E5%92%8C%E6%9C%AC%E8%B4%A8%E7%9F%A9%E9%98%B5/%E5%BD%92%E4%B8%80%E5%8C%96.png" alt="归一化"></p><p>上图左边是原始图像的坐标，右边是归一化后的坐标，H是归一化的变换矩阵，可记为如下形式：</p><p>$$<br>   T&#x3D;S<br>   \begin{bmatrix} 1&amp;0&amp;-\bar{u}\0&amp;1&amp;-\bar{v}\0&amp;0&amp;\frac{1}{S} \end{bmatrix}<br>$$<br>其中，$\bar{u},\bar{v}$是图像点坐标两个分量的平均值</p><p>$$<br>   \bar{u}&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^N u_i,\bar{v}&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^N v_i<br>$$</p><p>S表示尺度，其表达式为：</p><p>$$<br>   S&#x3D;\frac{\sqrt{2}\cdot N}{\sqrt{\sum_{i&#x3D;1}^N (u_i-\bar{u})^2+(v_i-\bar{v})^2}}<br>$$</p><p><strong>注：这里的公式很多网上的博客都错了，分子的N应该在根号外面，这样平均距离才是$\sqrt{2}$,OpenCV的八点法归一化函数也是这样做的</strong></p><p>这样，首先对原始的图像坐标进行归一化处理，再利用8点法求解基础矩阵，最后将求得的结果解除归一化，得到基础矩阵F，总结如下：</p><ul><li>对图像1进行归一化处理，计算一个只包含平移和缩放的变换$T_1$，将图像1中的匹配点集$p_i^1$变换到新的点集$\hat{p_i^1}$，新点集的形心位于原点$(0,0)^T$，并且它们到原点的平均距离是$\sqrt{2}$。</li><li>对图像2，计算变换矩阵$T_2$进行相同的归一化处理</li><li>使用8点法利用变换后的点集估计基础矩阵$\hat{F}$</li><li>建立变换$F&#x3D;T^T_2\hat{F} T_1$</li></ul><p>对单应矩阵的归一化处理同理，总结如下：</p><ul><li>对图像1进行归一化处理，计算一个只包含平移和缩放的变换$T_1$，将图像1中的匹配点集$p_i^1$变换到新的点集$\hat{p_i^1}$，新点集的形心位于原点$(0,0)^T$，并且它们到原点的平均距离是$\sqrt{2}$。</li><li>对图像2，计算变换矩阵$T_2$进行相同的归一化处理</li><li>使用8点法利用变换后的点集估计基础矩阵$\hat{H}$</li><li>建立变换$H&#x3D;T^{-1}_2 \hat{H}T_1$</li></ul><p>ORB-SLAM2中的归一化操作与论文中的不同，是采取了一阶矩进行归一化，具体来说就是缩放因子分别为</p><p>$$<br>   sX&#x3D;\frac{N}{\sum_{i&#x3D;1}^N \begin{vmatrix}u_i-\bar{u} \end{vmatrix}},<br>   sY&#x3D;\frac{N}{\sum_{i&#x3D;1}^N \begin{vmatrix}v_i-\bar{v} \end{vmatrix}}<br>$$</p><p>至于为什么这样做，还没搞明白。</p>]]></content>
    
    
    <summary type="html">&lt;center&gt;并没有看上去那么基础&lt;/center&gt;</summary>
    
    
    
    <category term="Computer Version" scheme="https://erenjaeger-01.github.io/categories/Computer-Version/"/>
    
    
    <category term="Computer Version" scheme="https://erenjaeger-01.github.io/tags/Computer-Version/"/>
    
  </entry>
  
  <entry>
    <title>计算机视觉系列（四）：单应矩阵</title>
    <link href="https://erenjaeger-01.github.io/2022/05/09/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E5%8D%95%E5%BA%94%E7%9F%A9%E9%98%B5/"/>
    <id>https://erenjaeger-01.github.io/2022/05/09/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E5%8D%95%E5%BA%94%E7%9F%A9%E9%98%B5/</id>
    <published>2022-05-09T13:06:17.000Z</published>
    <updated>2022-05-09T13:10:46.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="单应矩阵"><a href="#单应矩阵" class="headerlink" title="单应矩阵"></a>单应矩阵</h3><p>单应矩阵约束了同一3D空间点在两个像素平面上的2D坐标，本质上是射影平面上的可逆齐次线性变换。对于特征点对$p_1,p_2$，用单应矩阵$H21$描述特征点对之间的变换关系，有：</p><p>$$\begin{equation}<br>    p_2&#x3D;H_{21}*p_1 \tag{1}<br>\end{equation}$$</p><p>其中$H_{21}$中的“21”表示从1到2的单应矩阵，也就是指“把坐标系2中的向量变换到坐标系1中”。下称矩阵形式，有</p><p>$$<br> \begin{equation}<br>    \begin{bmatrix} u_2\v_2\1 \end{bmatrix}&#x3D;<br>    \begin{bmatrix} h_1&amp;h_2&amp;h_3\h_4&amp;h_5&amp;h_6\h_7&amp;h_8&amp;h_9 \end{bmatrix}<br>    \begin{bmatrix} u_1\v_1\1 \end{bmatrix} \tag{2}<br> \end{equation}<br>$$</p><p> 两边左侧叉乘$p_2$，则等式左侧为0，右侧向量叉乘转换为反对称矩阵，得到下式</p><p> $$<br> \begin{equation}<br>    \begin{bmatrix} 0&amp;-1&amp;v_2\1&amp;0&amp;-u_2\-v_2&amp;u_2&amp;0 \end{bmatrix}<br>    \begin{bmatrix} h_1&amp;h_2&amp;h_3\h_4&amp;h_5&amp;h_6\h_7&amp;h_8&amp;h_9 \end{bmatrix}<br>    \begin{bmatrix} u_1\v_1\1 \end{bmatrix}&#x3D;0<br>    \tag{3}<br> \end{equation}<br>$$</p><p>展开计算，有</p><p>$$   u_2&#x3D;(h_1<em>u_1+h_2</em>v_1+h_3)&#x2F;(h_7<em>u_1+h_8</em>v_1+h_9) $$<br>$$<br>\begin{equation}<br>   v_2&#x3D;(h_4<em>u_1+h_5</em>v_1+h_6)&#x2F;(h_7<em>u_1+h_8</em>v_1+h_9)<br>   \tag{4}<br>\end{equation}<br>$$</p><p>两边乘分母，得到</p><p>$$   h_1<em>u_1+h_2</em>v_1+h_3-(h_7<em>u_1</em>u_2+h_8<em>v_1</em>u_2+h_9<em>u_2)&#x3D;0 $$<br>$$<br>\begin{equation}<br>   -(h_4</em>u_1+h_5<em>v_1+h_6)+(h_7</em>u_1<em>v_2+h_8</em>v_1<em>v_2+h_9</em>v_2)&#x3D;0<br>   \tag{5}<br>\end{equation}<br>$$</p><p>转化为矩阵形式，有</p><p>$$<br> \begin{equation}<br>    \begin{bmatrix} u_1&amp;v_1&amp;1&amp;0&amp;0&amp;0&amp;-u_1<em>u_2&amp;-v_1</em>u_2&amp;-u_2\0&amp;0&amp;0&amp;-u_1&amp;-v_1&amp;-1&amp;u_1<em>v_2&amp;v_1</em>v_2&amp;v_2 \end{bmatrix}<br>    \begin{bmatrix} h_1\h_2\h_3\h_4\h_5\h_6\h_7\h_8\h_9 \end{bmatrix}&#x3D;0<br>    \tag{6}<br> \end{equation}<br>$$</p><p>令第一项为$A$,第二项为$X$，则有</p><p>$$<br> \begin{equation}<br>    AX&#x3D;0<br>    \tag{7}<br> \end{equation}<br>$$</p><p>这样，每对特征点提供两个约束，$H$共有9个元素，由于变换是齐次的，所以同一个单应矩阵$H$可以相差一个非零常数因子，因此一个单应矩阵有8个自由度，理论上提供四对点形成8个约束方程就可以求解。</p><h3 id="通过单应矩阵恢复位姿-R-t"><a href="#通过单应矩阵恢复位姿-R-t" class="headerlink" title="通过单应矩阵恢复位姿$R,t$"></a>通过单应矩阵恢复位姿$R,t$</h3><p>从定义出发，单应矩阵通常描述处于共同平面的一些点在两张图像之间的变换关系。设图像$I_1,I_2$有一对匹配好的特征点$p_1,p_2$，这个特征点落在平面P上，设这个平面满足方程</p><p>$$<br>   n^TP+d&#x3D;0<br>$$</p><p>整理得到</p><p>$$<br>   -\frac{n^TP}{d}&#x3D;1<br>$$</p><p>又因为</p><p>$$<br>   s_1p_1&#x3D;KP,s_2p_2&#x3D;K(RP+t)<br>$$</p><p>有</p><p>$$<br>   p_2\simeq K(RP+t)\simeq K(RP+t \cdot (-\frac{n^TP}{d}))\simeq K(R-\frac{n^T}{d}P\simeq K(R-\frac{tn^T}{d})K^{-1}p_1<br>$$</p><p>从上面可以看出，单应矩阵$H_{21}$就是等式右边那一坨,设那一坨是A，A就包含了$R，t$的信息。</p><p>对H进行奇异值分解，会得到8组解$R,T$，选出3D点在相机前方最多的解为最优解</p><p>Q：为什么是八组解？</p><p>A：已知约束条件</p><ul><li>$d_1\ge d_2 \ge d_3$</li><li>$\sum_{i&#x3D;1}^3x_i^2&#x3D;1$</li></ul><p>其中，$d_i$是奇异值，$x_i$是单位法向量n的坐标，所以平方和为1.</p><p>根据上面条件求解线性方程组，使其有非零解</p><p>$$<br>\left{\begin{array}{l}<br>\left(d^{\prime 2}-d_{2}^{2}\right) x_{1}^{2}+\left(d^{\prime 2}-d_{1}^{2}\right) x_{2}^{2}&#x3D;0 \<br>\left(d^{\prime 2}-d_{3}^{2}\right) x_{2}^{2}+\left(d^{\prime 2}-d_{2}^{2}\right) x_{3}^{2}&#x3D;0 \<br>\left(d^{\prime 2}-d_{1}^{2}\right) x_{3}^{2}+\left(d^{\prime 2}-d_{3}^{2}\right) x_{1}^{2}&#x3D;0<br>\end{array}\right.<br>$$</p><p>那么根据线性代数的知识可以知道，该线性方程组的行列式必须为0，即</p><p>$$<br>   (d^{‘2}-d_1^2)(d^{‘2}-d_2^2)(d^{‘2}-d_3^2)&#x3D;0<br>$$</p><p>逐个分析$d^{‘}$的取值</p><ul><li><p>$d^{‘}&#x3D;\pm d_1$</p><p> 唯一解为:$x_1&#x3D;x_2&#x3D;x_3&#x3D;0$，不满足第二个条件</p></li><li><p>$d^{‘}&#x3D;\pm d_3$</p><p> 唯一解为:$x_1&#x3D;x_2&#x3D;x_3&#x3D;0$，不满足第二个条件</p></li><li><p>$d^{‘}&#x3D;\pm d_2且d_1 \ne d_3$</p><p> 有四组解：</p></li></ul><p>$$<br>\left{\begin{array}{l}<br>x_{1}&#x3D;\varepsilon_{1} \sqrt{\frac{d_{1}^{2}-d_{2}^{2}}{d_{1}^{2}-d_{3}^{2}}} \<br>x_{2}&#x3D;0 \<br>x_{3}&#x3D;\varepsilon_{3} \sqrt{\frac{d_{2}^{2}-d_{3}^{2}}{d_{1}^{1}-d_{3}^{2}}}<br>\end{array}\right.<br>$$</p><p>又，对于平移向量的公式</p><ul><li>$d^{‘}&gt;0$</li></ul><p>$$<br>   t^{‘}&#x3D;(d_1-d_3)\begin{pmatrix}<br>      x_1 \ 0 \ -x_3<br>   \end{pmatrix}<br>$$</p><ul><li>$d^{‘}&lt;0$</li></ul><p>$$<br>   t^{‘}&#x3D;(d_1+d_3)\begin{pmatrix}<br>      x_1 \ 0 \ x_3<br>   \end{pmatrix}<br>$$</p><p>可以看出，平移向量和每组解都有联系，因此就成了2*4&#x3D;8组解，选出3D点在相机前方最多的解为最优解。</p>]]></content>
    
    
    <summary type="html">&lt;center&gt;描述了同一个平面上的点在两幅图像之间的映射关系&lt;/center&gt;</summary>
    
    
    
    <category term="Computer Version" scheme="https://erenjaeger-01.github.io/categories/Computer-Version/"/>
    
    
    <category term="Computer Version" scheme="https://erenjaeger-01.github.io/tags/Computer-Version/"/>
    
  </entry>
  
  <entry>
    <title>ORB-SLAM学习（二）：地图初始化</title>
    <link href="https://erenjaeger-01.github.io/2022/05/08/ORB-SLAM%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%9C%B0%E5%9B%BE%E5%88%9D%E5%A7%8B%E5%8C%96/"/>
    <id>https://erenjaeger-01.github.io/2022/05/08/ORB-SLAM%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%9C%B0%E5%9B%BE%E5%88%9D%E5%A7%8B%E5%8C%96/</id>
    <published>2022-05-08T03:41:18.000Z</published>
    <updated>2022-05-10T03:04:54.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h2><p>提取完ORB特征点后，需要对地图进行初始化操作。如果是单目相机，由于图像中不包含深度信息，需要通过连续的几帧图像计算出尺度信息，构建初始的三维点云。如果是双目或RGB-D相机，本身的图像就含有深度信息，因此第一帧的时候就可以完成初始化操作。下面主要对单目相机的初始化进行分析。</p><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><p><a href="https://kxzhang.cn/2022/05/09/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A9%E9%98%B5%E5%92%8C%E6%9C%AC%E8%B4%A8%E7%9F%A9%E9%98%B5/#more">基础矩阵，本质矩阵</a></p><p><a href="https://kxzhang.cn/2022/05/09/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E5%8D%95%E5%BA%94%E7%9F%A9%E9%98%B5/">单应矩阵</a></p><p><a href="https://kxzhang.cn/2022/05/09/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/#more">奇异值分解</a></p><p><a href="https://kxzhang.cn/2022/05/09/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C/#more">卡方检验</a></p><h2 id="地图初始化"><a href="#地图初始化" class="headerlink" title="地图初始化"></a>地图初始化</h2><p>地图初始化在<em>Track()<em>函数中，首先判断</em>mstate</em>有没有初始化,如果没被初始化就根据传感器类型进入不同的初始化函数中，单目初始化函数为*MonocularInitialization()<em>，双目或RGB-D相机共用一个初始化函数</em>StereoInitialization()*。</p><h3 id="StereoInitialization"><a href="#StereoInitialization" class="headerlink" title="StereoInitialization()"></a><em>StereoInitialization()</em></h3><p>首先判断单目初始器<em>mpInitializer</em>有没有被创建，没有并且这一帧图像的特征点数目超过100个，就创建<em>mpInitializer</em>，并且记录参考帧数据。</p><p>如果<em>mpInitializer</em>被创建了，但是这一帧的特征点数目没超过100，就删除<em>mpInitializer</em>从头再来。如果一切条件满足，就开始进行特征点匹配，这一步通过<em>SearchForInitialization()<em>函数实现，函数运行完之后</em>mvbPrevMatched</em>存储匹配好的特征点，<em>mvIniMatches</em>是个vector容器，索引值表示参考帧的关键点索引值，索引值对应的值表示当前帧的关键点索引值，通过这个保存特征点的匹配关系。</p><p>特征匹配之后调用<em>mpInitializer</em>对象的*Initialize()*函数进行单目初始化。</p><h3 id="成员变量初始化"><a href="#成员变量初始化" class="headerlink" title="成员变量初始化"></a>成员变量初始化</h3><p>遍历一遍匹配好的特征点，把两帧之间匹配好的特征点储存在<em>mvMatches12</em>变量中，把是否匹配成功标志存入<em>mvbMatched1</em>中，成功为true，失败为false。</p><h3 id="RANSAC随机采样"><a href="#RANSAC随机采样" class="headerlink" title="RANSAC随机采样"></a>RANSAC随机采样</h3><ul><li>设置随机数种子</li><li>进行两百次迭代，每次迭代找出随机的八个点，用于后续的八点法求单应矩阵和基础矩阵，把每次找出的特征点索引储存在<em>mvSets</em>中，之后从待搜索的范围内删除该特征点的索引，确保不会被重复采集到</li></ul><h3 id="计算基础矩阵F和单应矩阵H"><a href="#计算基础矩阵F和单应矩阵H" class="headerlink" title="计算基础矩阵F和单应矩阵H"></a>计算基础矩阵F和单应矩阵H</h3><p>这里为了加速开了两个线程，下面分开介绍</p><h4 id="计算单应矩阵H"><a href="#计算单应矩阵H" class="headerlink" title="计算单应矩阵H"></a>计算单应矩阵H</h4><p>在函数*FindHomography()*中</p><p>首先进行对特征点进行归一化操作，这样尺度就获得了统一，计算结果具有更高的准确性。有关归一化的操作见基础知识。接下来对之前RANSAC的两百次随机采样的结果进行循环计算。在两百次循环中，干了一下这些事情：</p><ul><li>通过*ComputeH21()*函数计算归一化后的单应矩阵，计算完成后再通过之前归一化的转换矩阵的逆矩阵缩放回真实的单应矩阵。</li><li>利用重投影误差对当次的RABSAC结果进行评分，计算重投影并评分调用了*CheckHomography()*函数。</li><li>保存评分最高的单应矩阵并退出循环返回。</li></ul><p>关于*CheckHomography()*函数：</p><p>初始化操作中协方差系数<em>sigma</em>设置为1.0，因为只有一层图像</p><p>在单应矩阵中，重投影误差定义为从当前帧图像投影到参考帧图像的特征点匹配度和从参考帧图像投影到当前帧图像的特征点匹配度，具体表现为根据所求的单应矩阵求出从图1到图2的特征点，再计算投影到图2的点和图2对应的特征点之间的距离。</p><p>误差计算玩后和卡方检验的阈值进行对比，如果大于阈值就舍去，小于阈值就得分累加，就这样计算两遍（图1到图2和从图2到图1），最后返回得分。</p><h4 id="计算基础矩阵F"><a href="#计算基础矩阵F" class="headerlink" title="计算基础矩阵F"></a>计算基础矩阵F</h4><p>在函数*FindFundamental()*中</p><p>首先进行对特征点进行归一化操作，这样尺度就获得了统一，计算结果具有更高的准确性。有关归一化的操作见基础知识。接下来对之前RANSAC的两百次随机采样的结果进行循环计算。在两百次循环中，干了这些事情：</p><ul><li>通过*ComputeF21()*函数计算归一化后的单应矩阵，计算完成后再通过之前归一化的转换矩阵的逆矩阵缩放回真实的基础矩阵。</li><li>利用重投影误差对当次的RABSAC结果进行评分，计算重投影并评分调用了*CheckFundamental()*函数。</li><li>保存评分最高的基础矩阵并退出循环返回。</li></ul><p>关于*CheckFundamental()*函数：</p><p>初始化操作中协方差系数<em>sigma</em>设置为1.0，因为只有一层图像</p><p>在基础矩阵中，重投影误差定义为从当前帧图像投影到参考帧图像的特征点到极线的距离和从参考帧图像投影到当前帧图像的特征点到极线的距离，误差期望为0，到极线距离越远误差越大。</p><p>误差计算玩后和卡方检验的阈值进行对比，如果大于阈值就舍去，小于阈值就得分累加，就这样计算两遍（图1到图2和从图2到图1），最后返回得分。</p><h3 id="根据得分判断是用H还是F求解位姿"><a href="#根据得分判断是用H还是F求解位姿" class="headerlink" title="根据得分判断是用H还是F求解位姿"></a>根据得分判断是用H还是F求解位姿</h3><p>得分计算公式：</p><p>$$<br>    RH&#x3D;\frac{SH}{SH+SF}<br>$$</p><p>SH是单应矩阵的评分，SF是基础矩阵的评分，如果RH&gt;0.4,就从单应矩阵恢复$R,t$，如果RH&lt;0.4，就从基础矩阵恢复$R,t$。恢复$R,t$的函数分别为*ReconstructH()<em>和</em>ReconstructF()*。</p><h4 id="ReconstructH"><a href="#ReconstructH" class="headerlink" title="ReconstructH()"></a><em>ReconstructH()</em></h4><p>通过单应矩阵恢复位姿矩阵，有公式</p><p>$$<br>   H&#x3D;K(R-t\frac{n}{d})K^{-1}<br>$$</p><p>其中K表示相机内参矩阵，n表示平面法向量，令中间的部分为$A$，则有</p><p>$$<br>   A&#x3D;K^{-1}HK<br>$$</p><p>这样求出矩阵$A$，对其进行奇异值分解，得到</p><p>$$<br>   A&#x3D;UwV^{T}<br>$$</p><p>得到8组解,<br>对8组解进行验证，选择产生相机前方最多3d点的一组解作为最优解并计算好点的数目。关于好点数目的计算调用了*CheckRT()*函数，最终把最优解的相关变量更新</p><p>关于*CheckRT()*函数：</p><p>首先将参考帧的坐标系定为世界坐标系，然后对所有的特征点对进行遍历，遍历做了以下行为：</p><ul><li>通过*Triangulate()*函数恢复3D点，该函数是利用三角化对点的深度进行估计；</li><li>检查通过三角化3D生成的三维点坐标是否无穷远，如果是无穷远点就跳过；</li><li>检查三维点的深度，求三维点和相机光心$O_1,O_2$的距离，同时根据余弦定理计算视差，如果深度小于0并且视差角过小也跳过；</li><li>计算三维点投影到两幅图像的重投影误差，这里直接通过针孔相机模型的公式计算，即</li></ul><p>$$<br>   \begin{bmatrix} u\v\1 \end{bmatrix}&#x3D;\frac{1}{Z}<br>   \begin{bmatrix} f_x&amp;0&amp;c_x\0&amp;f_y&amp;c_y\0&amp;0&amp;1 \end{bmatrix}<br>   \begin{bmatrix} X\Y\Z \end{bmatrix}<br>$$</p><p>如果重投影误差太大，就跳过；</p><ul><li>累加好点（经过重重筛选幸存下来）的个数。</li></ul><p>如果好点数目大于0，就选一个较小的视差角并返回好点个数。如果没有好点就把视差角设0。</p><h4 id="ReconstructF"><a href="#ReconstructF" class="headerlink" title="ReconstructF()"></a><em>ReconstructF()</em></h4><p>通过分解本质矩阵计算得到$R,t$，该操作调用函数*DecomposeE()*。根据本质矩阵分解原理会得到四组不同的解，但是只有一组是对的，所以要通过筛选进行判断。</p><p>通过*CheckRT()*检验得到的四组解，并选出最多好点的一组解进行判断。</p><p>如果好点数目优势不够明显就返回失败，如果通过考验就把相关解保存下来并指示初始化成功。</p><h3 id="创建初始化地图点"><a href="#创建初始化地图点" class="headerlink" title="创建初始化地图点"></a>创建初始化地图点</h3><p>如果上一步初始化成功了，首先把没有三角化的匹配点删除，然后将初始化后的第一帧作为世界坐标系，最后进行下列操作：</p><ul><li>将初始关键帧的描述子转换为词袋；</li><li>将关键帧插入地图；</li><li>遍历初始化的三维点，为这些三维点添加属性（还没学到这），最后插入到地图中，初始化完成。</li></ul>]]></content>
    
    
    <summary type="html">&lt;center&gt;单目相机多捞啊&lt;/center&gt;</summary>
    
    
    
    <category term="SLAM" scheme="https://erenjaeger-01.github.io/categories/SLAM/"/>
    
    
    <category term="SLAM" scheme="https://erenjaeger-01.github.io/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>计算机视觉系列（三）：ORB特征点</title>
    <link href="https://erenjaeger-01.github.io/2022/05/05/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9AORB%E7%89%B9%E5%BE%81%E7%82%B9/"/>
    <id>https://erenjaeger-01.github.io/2022/05/05/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9AORB%E7%89%B9%E5%BE%81%E7%82%B9/</id>
    <published>2022-05-05T11:17:47.000Z</published>
    <updated>2022-05-05T11:42:48.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="图像的特征点"><a href="#图像的特征点" class="headerlink" title="图像的特征点"></a>图像的特征点</h2><p>图像的特征点可以理解为图像中比较有代表性的点，这些点在相机视角发生少量变化后会保持不变，于是我们就可以在各个图像中找到相同的点。在SLAM的视觉里程计中，我们需要通过一帧帧连续拍摄的图像来对相机的运动进行估计，因此需要对每一帧图像中提取出特征点，通过这些点对相机的位姿估计进行讨论。</p><h2 id="ORB特征点"><a href="#ORB特征点" class="headerlink" title="ORB特征点"></a>ORB特征点</h2><p><strong>ORB特征点（Oriented FAST and Rotated BRIEF）</strong>是对<strong>FAST关键点</strong>和<strong>BRIEF特征描述子</strong>的一种结合与改进。</p><ul><li>FAST关键点是一种角点，主要检测局部像素区域灰度变化明显的地方，因为只检测亮度，所以速度非常快；</li><li>BRIEF是一种二进制描述子，在提取完关键点后，对每个关键点计算描述子，用来描述关键点周围像素的信息，描述子的设计原则是“ <strong>外观相似的特征应该具有相似的描述子</strong>”。</li></ul><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9AORB%E7%89%B9%E5%BE%81%E7%82%B9/fast_speedtest.jpg" alt="FAST关键点"></p><p>虽然FAST关键点速度非常快，但是快有快的缺陷。首先，FAST关键点没有方向信息，因此当图像发生旋转之后，关键点对应的描述子会发生变化；此外，FAST关键点不具有尺度不变性，也就是说不同距离对着同一个物体拍照，远处看起来像关键点的地方，距离近了就可能不是关键点了。对于这两个问题，ORB特征点采用了如下方法进行解决。</p><ul><li>对于尺度问题，构建 <a href="https://kxzhang.cn/2022/05/01/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%9B%BE%E5%83%8F%E9%87%91%E5%AD%97%E5%A1%94/#more">图像金字塔</a>,得到不同尺度下的不同分辨率图像，对于每一层图像提取特征点，最后再匹配到原图像，这样就实现了关键点的尺度不变性；</li><li>对于旋转问题，ORB特征点采用了灰度质心法计算特征点的方向，通过特征点的方向实现关键点的旋转不变性。</li></ul><h2 id="举个例子"><a href="#举个例子" class="headerlink" title="举个例子"></a>举个例子</h2><p>OpenCV提取图像特征点</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;opencv2/opencv.hpp&gt;</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;chrono&gt;</span><br><span class="line">using namespace cv;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">#define EDGE_THRESHOLD 19</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    std::vector&lt;cv::KeyPoint&gt; keypoints;</span><br><span class="line">    Mat descriptors;</span><br><span class="line">    cv::Ptr&lt;cv::FeatureDetector&gt; detector = cv::ORB::create();</span><br><span class="line">    cv::Ptr&lt;cv::DescriptorExtractor&gt; descriptor = cv::ORB::create();</span><br><span class="line"></span><br><span class="line">    Mat image = imread(&quot;../distorted.png&quot;, IMREAD_COLOR);</span><br><span class="line">    //-- 第一步:检测 Oriented FAST 角点位置</span><br><span class="line">    detector-&gt;detect(image, keypoints);</span><br><span class="line"></span><br><span class="line">    //-- 第二步:根据角点位置计算 BRIEF 描述子</span><br><span class="line">    descriptor-&gt;compute(image, keypoints, descriptors);</span><br><span class="line"></span><br><span class="line">    Mat outimg;</span><br><span class="line">    drawKeypoints(image, keypoints, outimg, cv::Scalar::all(-1), cv::DrawMatchesFlags::DEFAULT);</span><br><span class="line">    imshow(&quot;原图&quot;, image);</span><br><span class="line">    imshow(&quot;OpenCV函数提取的ORB特征点&quot;, outimg);</span><br><span class="line">    imwrite(&quot;OpenCV提取的ORB特征点.jpg&quot;, outimg);</span><br><span class="line">    waitKey(0);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果：</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9AORB%E7%89%B9%E5%BE%81%E7%82%B9/distorted.png" alt="原图"></p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9AORB%E7%89%B9%E5%BE%81%E7%82%B9/OpenCV%E6%8F%90%E5%8F%96%E7%9A%84%E7%89%B9%E5%BE%81%E7%82%B9.jpg" alt="OpenCV提取的特征点"></p>]]></content>
    
    
    <summary type="html">&lt;center&gt;像素也分三六九等&lt;/center&gt;</summary>
    
    
    
    <category term="Computer Version" scheme="https://erenjaeger-01.github.io/categories/Computer-Version/"/>
    
    
    <category term="Computer Version" scheme="https://erenjaeger-01.github.io/tags/Computer-Version/"/>
    
  </entry>
  
  <entry>
    <title>ORB-SLAM学习（一）：ORB特征提取</title>
    <link href="https://erenjaeger-01.github.io/2022/05/05/ORB-SLAM%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9AORB%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    <id>https://erenjaeger-01.github.io/2022/05/05/ORB-SLAM%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9AORB%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/</id>
    <published>2022-05-05T07:50:44.000Z</published>
    <updated>2022-05-13T14:29:16.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="相关知识介绍"><a href="#相关知识介绍" class="headerlink" title="相关知识介绍"></a>相关知识介绍</h2><p><a href="https://kxzhang.cn/2022/05/01/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%9B%BE%E5%83%8F%E9%87%91%E5%AD%97%E5%A1%94/#more">图像金字塔</a></p><p><a href="https://kxzhang.cn/2022/05/05/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9AORB%E7%89%B9%E5%BE%81%E7%82%B9/">ORB特征点</a></p><h2 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h2><p>ORB-SLAM2中的ORB特征点提取和OpenCV有所不同。在ORB-SLAM2中进行了一系列操作让特征点均匀的分布在图像中，这样对于后面的位姿估计比较好。OpenCV直接提取的特征点可能会出现扎堆，集中等现象，这样很多特征点就没用了。下面对ORB-SLAM2中的ORB特征点提取的过程进行介绍。</p><h2 id="构建图像金字塔"><a href="#构建图像金字塔" class="headerlink" title="构建图像金字塔"></a>构建图像金字塔</h2><ul><li><p>首先对图像进行扩展，这一步的目的是为了之后的高斯模糊操作。</p><p><img src="/./ORB-SLAM%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9AORB%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/%E5%9B%BE%E5%83%8F%E6%89%A9%E5%B1%95.png" alt="图像扩充示意，这一步是把原图像（最里边的矩形）扩展到最外侧的矩形"></p></li><li><p>进行一个循环，把每一层的图像都在上一层图像的基础上进行扩充（或者叫补边），该过程用到了OpenCV中的<em>copyMakeBorder</em>函数和<em>resize</em>函数，扩充的规则为<em>BORDER_REFLECT_101</em>，扩充的效果如下：</p></li></ul><p><img src="/./ORB-SLAM%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9AORB%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/%E5%9B%BE%E5%83%8F%E6%89%A9%E5%85%85.png" alt="左边是扩充效果，可以看出是一个类似镜像的效果"></p><h2 id="计算特征点并通过四叉树对特征点进行均匀化操作"><a href="#计算特征点并通过四叉树对特征点进行均匀化操作" class="headerlink" title="计算特征点并通过四叉树对特征点进行均匀化操作"></a>计算特征点并通过四叉树对特征点进行均匀化操作</h2><p>这一步又对每一层进行了一次遍历，干了这些事：</p><ul><li>把原图像的边扩充3个像素点，这个是为了进行FAST关键点提取预留的计算半径；</li><li>把图像均匀划分成了一堆小格子（均匀化）；</li><li>对这堆小格子进行遍历，在每个小格子中调用OpenCV中的FAST函数通过yaml文件中的设置阈值进行关键点提取，如果这都没采到哪怕是一个关键点，就采用最低的阈值再采一次；</li><li>把采取到的关键点坐标进行转换，之前是在小方格中的坐标系，现在要通过一些偏移操作转换到原图像下的坐标系；</li><li>调用*DistributeOctTree()*函数对特征点进行均匀化（ORB-SLAM的精髓之一），关于四叉树均匀化特征点可以通过下面一幅图来表示；</li></ul><p><img src="/./ORB-SLAM%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9AORB%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/%E5%9B%9B%E5%8F%89%E6%A0%91.png" alt="四叉树均匀化特征点"></p><ul><li>再进行一次坐标转换，这次是把提取到的关键点坐标转换为金字塔划分那一步的扩展图像的坐标；</li></ul><p>干完这些事情后，又进行了一次遍历（写到这才发现这么多遍历，难怪速度比OpenCV的库函数慢）</p><ul><li>计算特征点的方向信息，调用*IC_Angle()*函数，通过一些小操作提高了一些速度。</li></ul><h2 id="计算描述子并进行金字塔图像特征恢复"><a href="#计算描述子并进行金字塔图像特征恢复" class="headerlink" title="计算描述子并进行金字塔图像特征恢复"></a>计算描述子并进行金字塔图像特征恢复</h2><p>这一步一上来又对每层金字塔进行了一次遍历，然后把每层金字塔的特征点个数累加存到了一个变量里，之后根据这个变量设置了描述子矩阵的容量。</p><p>再来一次遍历（已经麻了），这一步开始计算描述子</p><ul><li>首先对金字塔图像进行高斯模糊，因为BRIEF描述子对噪声敏感，所以要通过高斯模糊消除噪声对描述子的影响，这一步就是用到了之前的图像扩充。高斯模糊方法采用的OpenCV函数*GaussianBlur()*。</li><li>然后对每个关键点计算描述子，首先要根据之前计算得到的方向对坐标进行旋转，坐标对齐之后根据描述子模板进行描述子计算，保存到<em>descriptors</em>矩阵中；</li><li>对于除第0层的图像（也就是最底下那一层），将所有的特征点乘相应层数的缩放因子，投射到最底层图像中，这样就实现了特征点的提取。</li></ul><h2 id="通过内参矩阵对特征点进行去畸变操作"><a href="#通过内参矩阵对特征点进行去畸变操作" class="headerlink" title="通过内参矩阵对特征点进行去畸变操作"></a>通过内参矩阵对特征点进行去畸变操作</h2><p>内参系数在yaml文件中都给写好了，直接调用OpenCV的库函数*undistortPoints()*进行特征点去畸变操作。</p><h2 id="将特征点分配到网格中"><a href="#将特征点分配到网格中" class="headerlink" title="将特征点分配到网格中"></a>将特征点分配到网格中</h2><p>调用<em>AssignFeaturesToGrid()</em> </p>]]></content>
    
    
    <summary type="html">&lt;center&gt;万里长征第一步&lt;/center&gt;</summary>
    
    
    
    <category term="SLAM" scheme="https://erenjaeger-01.github.io/categories/SLAM/"/>
    
    
    <category term="SLAM" scheme="https://erenjaeger-01.github.io/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>五一假期产生的一些感想</title>
    <link href="https://erenjaeger-01.github.io/2022/05/04/%E4%BA%94%E4%B8%80%E5%81%87%E6%9C%9F%E4%BA%A7%E7%94%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%84%9F%E6%83%B3/"/>
    <id>https://erenjaeger-01.github.io/2022/05/04/%E4%BA%94%E4%B8%80%E5%81%87%E6%9C%9F%E4%BA%A7%E7%94%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%84%9F%E6%83%B3/</id>
    <published>2022-05-04T08:43:01.000Z</published>
    <updated>2022-05-05T11:44:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>北京的疫情又严重起来了，五一假期全都在实验室里坐着，每天除了不上课和平时基本没什么区别。最近在学习SLAM，想把学习过程中的一些想法记录下来，又不想拘泥于以前的做法，于是搭建了这个博客网站。不得不说框架真的是很方便，对于我这种对前端一无所知的人来说也能很快的搭好一个看上去还不错（个人感觉）的网站。</p><p>时常觉得自己还挺幸运的，因为对自己的专业，也就是自动化并不讨厌，甚至还挺喜欢，这样起码在学习的过程中不会觉得特别痛苦。大一入学的时候接触到了一些科创活动，碰到了一些很强的人，之后就顺理成章的一路走下来了。从大一寒假决定转专业到自动化，再到之后参加的各种比赛，大四考研，最后到现在的研究生，每一步好像都没有什么犹豫。本科在实验室的日子也是我在南航少数几段有声有色的时光。</p><p>本科做了三年的RoboMaster，这是大疆举办的一个全国性的机器人比赛，大一第一次见到这比赛的时候惊为天人，一瞬间就知道了自己想做的就是这玩意。</p><p><img src="/./%E4%BA%94%E4%B8%80%E5%81%87%E6%9C%9F%E4%BA%A7%E7%94%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%84%9F%E6%83%B3/3.jpg" alt="RoboMaster"></p><p>没有半点犹豫，我大一就进去拧了几个月螺丝，同时看看学长是怎么做的，说实话，那段激情燃烧的岁月还是挺值得留恋的，年轻人精力旺盛，一下子熬个几天夜不成问题，半夜的时候七八个人点个夜宵（那时候外卖还是可以直接送到楼下的），拿麦克纳姆轮搭成的半成品车架子当餐桌，凌晨三四点走在回宿舍的路上，校园里静悄悄的，连走路的脚步声都听的很清晰。</p><p><img src="/./%E4%BA%94%E4%B8%80%E5%81%87%E6%9C%9F%E4%BA%A7%E7%94%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%84%9F%E6%83%B3/1.jpg" alt="大一实验室的工作台"></p><p><img src="/./%E4%BA%94%E4%B8%80%E5%81%87%E6%9C%9F%E4%BA%A7%E7%94%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%84%9F%E6%83%B3/2.jpg" alt="半夜实验室夜宵"></p><p>暑假的时候队伍选拔新队员，我整个暑假大部分时间都在学校，只有中间回去了几天，小小缅怀了一下高中母校。当时选拔的题目是做一个倒立摆，那个时候我开始正式的学习STM32单片机，然后发现自己真的是很菜，调个PID都要调好久，遑论之前的底层驱动配置了。暑假的最后二十天整个人进入到一种很纯粹的状态，除了吃饭睡觉洗澡其他时间全在实验室里，终于在验收前几天实现了全部功能。验收的时候我惊讶的发现同时参加选拔的人有三分之二没做出来或者放弃选拔了，其中不乏之前看起来很强的大佬，因为他们高中就接触过类似的东西，而我的高中除了做题打游戏其他的啥也没有。</p><p><img src="/./%E4%BA%94%E4%B8%80%E5%81%87%E6%9C%9F%E4%BA%A7%E7%94%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%84%9F%E6%83%B3/4.jpg" alt="倒立摆"></p><p>做倒立摆的经历算是让我对电控真正的入了个门，同时也意识到做技术好像没有想象中的那么难，电脑也不全是用来打游戏的（误）。</p><p>新学期，我逐渐热衷于参加各种电子设计比赛，当时有一种冲动，想要通过做这些比赛来证明自己，现在看来做比赛有利有弊，好处是确实可以让你在比较短的时间内学到很多东西，坏处是很多东西并没有完全搞懂，浮于表面，毕竟做比赛拿来用，能解决问题就完事了。这个想法给我后面埋了很多坑，最突出的就是大四下学期出去实习画的板子前两版不能用。</p><p>大二一开始参加了校电赛，当时做了一个灯光遥控装置，刚一进去就拿了一等奖，这个比赛让我的自信心再一次爆炸，学习的欲望也随之更加旺盛。</p><p><img src="/./%E4%BA%94%E4%B8%80%E5%81%87%E6%9C%9F%E4%BA%A7%E7%94%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%84%9F%E6%83%B3/5.jpg" alt="校电赛"></p><p>因为这些比赛，我的大一和大二开始这段时间可谓是顺风顺水，大一学科成绩也在前10%，还有本硕连读资格，好像保研只是水到渠成的事情。当然，人生总是充满了戏剧性，校电赛结束后，我得了肺结核。</p><p>这场病直接毁了之前的一切。我在医院住了一个月，休学了两个月，大二上学期的课程上了一半全部退掉不让考试，堆叠到之后的学期。当医生和我说要住院一个月时我哭的梨花带雨，因为我担心这样会失去保研资格，现在想想当时的想法还挺好笑的。</p><p>大二下学期重新回到学校，课程负担已经因为转专业和休学，同时我又拒绝留级而增加到了一个恐怖的高度，我意识到我无论如何也不可能掌握全部的课程，同时还要做RoboMaster比赛，花一个暑假入的队不能就这么退了，所以我只认真学了在我看来有用的课，比如数电模电，比如信号与线性系统，比如计算机软硬件基础，毛概之类的课能不上就不上，这样整个学期的节奏就还在一个可以接受的范围内。最终我的综合成绩很差，顺理成章的失去了保研资格，但是我认真学了的几门课成绩都还可以，我觉得想要的知识学到就够了。</p><p>大二的RoboMaster我负责了一台步兵机器人的电控，这个比赛也是大二下学期做的，这一年的机器人都做得非常好，平时测试各种移动，自瞄都很稳，但是最后比赛的时候超级电容出了问题，这是整个队伍都疏忽的一点，挺可惜，但是也没有办法，只能认了。</p><p><img src="/./%E4%BA%94%E4%B8%80%E5%81%87%E6%9C%9F%E4%BA%A7%E7%94%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%84%9F%E6%83%B3/9.jpg" alt="步兵机器人"></p><p><img src="/./%E4%BA%94%E4%B8%80%E5%81%87%E6%9C%9F%E4%BA%A7%E7%94%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%84%9F%E6%83%B3/10.jpg" alt="战队合影"></p><p>可能生病只是一个开始，回来后我做的所有事情好像没有一件是成功的。大二暑假全国电赛寄了，本来想靠这个说不定能保研，大三做智能车也寄了，本来想靠这个说不定也能保研。每次校内赛选拔成绩都很好，比如校电赛和智能车的校内赛，智能车校内赛还拿了特等奖，最终比赛都因为莫名其妙的原因出了岔子。智能车另一个组的人拿了我校赛的代码，最后拿了国三。就这样磕磕绊绊过完了整个大三，八月份的时候决定考研。</p><p><img src="/./%E4%BA%94%E4%B8%80%E5%81%87%E6%9C%9F%E4%BA%A7%E7%94%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%84%9F%E6%83%B3/11.jpg" alt="智能车"></p><p>考研那段时间仿佛又找回了从前当小镇做题家的日子，本科做的最后一件算是大一点的事没有出岔子，我如愿以偿来到了北航，然后意识到以前做的东西实验室基本上都不做（大悲），不过学新东西的感觉很好，未来的路也算是明了，起码自己想做什么方向还是有数的，这个程度我已经觉得算是满意了。</p><p>假期最后一天的晚上胡思乱想，翻来覆去睡不着，眼睛瞪得跟铜铃一样，脑子里就在回想这些东西。如果我因为从前的无法无天，不思进取要受到惩罚的话，那么我本科已经为此付出了相应的代价。研究生三年说不定就是学生生涯的结束了，希望这三年（只剩下两年多一点了）能够成长到想要的高度。</p><p>继续加油。</p>]]></content>
    
    
    <summary type="html">&lt;center&gt;放假放了个寂寞&lt;/center&gt;</summary>
    
    
    
    <category term="杂记" scheme="https://erenjaeger-01.github.io/categories/%E6%9D%82%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>计算机视觉系列（一）：图像金字塔</title>
    <link href="https://erenjaeger-01.github.io/2022/05/01/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%9B%BE%E5%83%8F%E9%87%91%E5%AD%97%E5%A1%94/"/>
    <id>https://erenjaeger-01.github.io/2022/05/01/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%9B%BE%E5%83%8F%E9%87%91%E5%AD%97%E5%A1%94/</id>
    <published>2022-05-01T10:11:28.000Z</published>
    <updated>2022-05-05T11:44:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="图像金字塔"><a href="#图像金字塔" class="headerlink" title="图像金字塔"></a>图像金字塔</h2><p>一幅图像的金字塔是一系列以金字塔形状排列，分辨率逐渐降低且源于同一张原始图的图像集合。金字塔的底部是待处理图像的高分辨率表示，而顶部是低分辨率的近似。层级越高，图像越小，分辨率越低。图像金字塔是图像中多尺度表达的一种，最初用于机器视觉和图像压缩，最主要功能用于图像分割，是一种以多分辨率来解释图像的有效但概念简单的结构。</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%9B%BE%E5%83%8F%E9%87%91%E5%AD%97%E5%A1%94/1.jpg" alt="pyramid"></p><p>生成图像金字塔主要有两种方式： <strong>向下采样</strong>和<strong>向上采样</strong>。</p><ul><li>向下采样：将图像从最底层（即上图中的level0）转换为level1、level2…的过程，图像分辨率不断降低。</li><li>向上采样：将图像从最顶层（即上图中的level4）转换为level3、level2…的过程，图像分辨率不断增大。</li></ul><p>常见的金字塔一般有两类：</p><ul><li><strong>高斯金字塔</strong>: 用来向下&#x2F;降采样，主要的图像金字塔；</li><li><strong>拉普拉斯金字塔</strong>: 用来从金字塔低层图像重建上层未采样图像，在数字图像处理中也即是预测残差，可以对图像进行最大程度的还原，配合高斯金字塔一起使用。</li></ul><p>在OpenCv中提供了对图像进行上下采样的接口：**pyrUp()<strong>和</strong>pyrDown()<strong>，同时提供了一个对图像进行尺度变换的函数</strong>resize()**。</p><p>获取金字塔一般来说包括两个步骤：</p><ul><li>对于向下采样，首先对图像进行高斯平滑，然后进行降采样（将图像尺寸行和列方向缩减一半）；</li><li>对于向上采样，首先对图像进行升采样（将图像尺寸行和列方向增大一倍），然后进行高斯平滑；</li></ul>]]></content>
    
    
    <summary type="html">&lt;center&gt;只是长得像金字塔&lt;/center&gt;</summary>
    
    
    
    <category term="Computer Version" scheme="https://erenjaeger-01.github.io/categories/Computer-Version/"/>
    
    
    <category term="Computer Version" scheme="https://erenjaeger-01.github.io/tags/Computer-Version/"/>
    
  </entry>
  
  <entry>
    <title>计算机视觉系列（二）：图像梯度</title>
    <link href="https://erenjaeger-01.github.io/2022/05/01/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%9B%BE%E5%83%8F%E6%A2%AF%E5%BA%A6/"/>
    <id>https://erenjaeger-01.github.io/2022/05/01/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%9B%BE%E5%83%8F%E6%A2%AF%E5%BA%A6/</id>
    <published>2022-05-01T03:16:01.000Z</published>
    <updated>2022-05-05T11:44:02.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="图像梯度的概念"><a href="#图像梯度的概念" class="headerlink" title="图像梯度的概念"></a>图像梯度的概念</h2><p>图像梯度是指图像某像素在x和y两个方向上的变化率（与相邻像素比较），是一个二维向量，由2个分量组成X轴的变化、Y轴的变化。其中：</p><ul><li>X轴的变化是指当前像素右侧（X加1）的像素值减去当前像素左侧（X减1）的像素值；</li><li>Y轴的变化是当前像素下方（Y加1）的像素值减去当前像素上方（Y减1）的像素值；<br>计算出来这两个分量，会形成一个二维向量，该向量描述了当前像素点的梯度。对这个向量取反正切函数 <em><strong>arctan</strong></em> ，可以得到梯度的角度。</li></ul><h2 id="图像梯度的求解"><a href="#图像梯度的求解" class="headerlink" title="图像梯度的求解"></a>图像梯度的求解</h2><p>图像梯度的求解过程可以用一个卷积核来实现：[-1,0,1]。</p><p>$\nabla{f(x,y)}&#x3D;$ $\left [ \begin{matrix} g_x\g_y \end{matrix} \right]&#x3D;$ $\left [ \begin{matrix} \frac{\partial f}{\partial x}\ \frac{\partial f}{\partial y} \end{matrix} \right]&#x3D;$ $\left [ \begin{matrix} f(x+1,y)-f(x-1,y)\f(x,y+1)-f(x,y-1) \end{matrix} \right]$</p><p><img src="/./%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%9B%BE%E5%83%8F%E6%A2%AF%E5%BA%A6/2.png" alt="gradient"></p><p>$\nabla{f(x,y)}&#x3D;$ $\left [ \begin{matrix} f(x+1,y)-f(x-1,y)\f(x,y+1)-f(x,y-1) \end{matrix} \right]&#x3D;$ $\left [ \begin{matrix} 55-105\90-40 \end{matrix} \right]&#x3D;$ $\left [ \begin{matrix} -50\ 50 \end{matrix} \right]$</p><p>图像梯度的绝对值为：</p><p>$\sqrt{50^2+(-50)^2}&#x3D;70.7107$</p><p>图像梯度的角度为：</p><p>$\arctan(-50&#x2F;50)&#x3D;-45^\circ$</p>]]></content>
    
    
    <summary type="html">&lt;center&gt;什么地方都会用到&lt;/center&gt;</summary>
    
    
    
    <category term="Computer Version" scheme="https://erenjaeger-01.github.io/categories/Computer-Version/"/>
    
    
    <category term="Computer Version" scheme="https://erenjaeger-01.github.io/tags/Computer-Version/"/>
    
  </entry>
  
  <entry>
    <title>OpenCV中用到的一些函数说明</title>
    <link href="https://erenjaeger-01.github.io/2022/05/01/OpenCV%E4%B8%AD%E7%94%A8%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E5%87%BD%E6%95%B0%E8%AF%B4%E6%98%8E/"/>
    <id>https://erenjaeger-01.github.io/2022/05/01/OpenCV%E4%B8%AD%E7%94%A8%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E5%87%BD%E6%95%B0%E8%AF%B4%E6%98%8E/</id>
    <published>2022-05-01T00:46:38.000Z</published>
    <updated>2022-05-05T11:42:50.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="取整函数"><a href="#取整函数" class="headerlink" title="取整函数"></a>取整函数</h2><p>cvRound()、cvFloor()、cvCeil()函数</p><ul><li>cvRound(): 返回参数最接近的整数值，四舍五入；</li><li>cvFloor(): 返回不大于参数的最大整数值，即向下取整；</li><li>cvCeil(): 返回不小于参数的最小整数值，即向上取整；</li></ul><p>示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;opencv2/opencv.hpp&gt;</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace cv;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    cout &lt;&lt; &quot;cvRound(2.5) : &quot; &lt;&lt; cvRound(2.5) &lt;&lt; endl;</span><br><span class="line">    cout &lt;&lt; &quot;cvFloor(2.5) : &quot; &lt;&lt; cvFloor(2.5) &lt;&lt; endl;</span><br><span class="line">    cout &lt;&lt; &quot;cvCeil(2.5)  : &quot; &lt;&lt; cvCeil(2.5)  &lt;&lt; endl;</span><br><span class="line">    </span><br><span class="line">    cout &lt;&lt; &quot;cvRound(2.5) : &quot; &lt;&lt; cvRound(2.5) &lt;&lt; endl;</span><br><span class="line">    cout &lt;&lt; &quot;cvFloor(2.5) : &quot; &lt;&lt; cvFloor(2.5) &lt;&lt; endl;</span><br><span class="line">    cout &lt;&lt; &quot;cvCeil(2.5)  : &quot; &lt;&lt; cvCeil(2.5)  &lt;&lt; endl;</span><br><span class="line">    </span><br><span class="line">    waitKey(0);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果如下</p><p><img src="/./OpenCV%E4%B8%AD%E7%94%A8%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E5%87%BD%E6%95%B0%E8%AF%B4%E6%98%8E/1.png" alt="opencv"></p>]]></content>
    
    
    <summary type="html">&lt;center&gt;OpenCV中用到的一些函数功能说明&lt;/center&gt;</summary>
    
    
    
    <category term="OpenCV" scheme="https://erenjaeger-01.github.io/categories/OpenCV/"/>
    
    
    <category term="OpenCV" scheme="https://erenjaeger-01.github.io/tags/OpenCV/"/>
    
  </entry>
  
</feed>
